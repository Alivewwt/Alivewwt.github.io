---
layout:     post
title:      "基于预训练模型的文本摘要"
subtitle:   "Text Summarization with Pretrained Encoders"
date:       2020-05-16 10:00:00
author:     "Wwt"
header-img: "img/bertsum/bg.png"
catalog: true
tags:   
    - 文本摘要
    - NLP
---
### 文本摘要任务

**任务概述**：浏览一篇新闻或论文，用精炼且核心的语句来概括整篇文章的内容，用户只需通过阅读摘要就能获取文章的主要内容。

文本摘要通过生成方式可以分为**生成式**和**抽取式**，根据处理对象的不同又可以分为**单文档**摘要和**多文档**摘要。

- 抽取式摘要主要从原文中抽取若干个句子作为摘要，通过对句子进行打分，获得句子的重要性，在通顺程度上要比生成式效果好，但也会出现冗余的问题。
- 生成式摘要与人类通过阅读文章，然后组织一些核心词和语句来生成摘要方式类似。主要存在一些未登录词、生成内容与原文不符合的问题。

### 摘要评测方法

文本摘要属于文本生成任务范畴，因此不能用简单的准召率来评测。当前比较常用的评测文本生成的方法大致是BLUE、ROUGE等。这些方法的缺点在于评测质量还比不上人工，只是从基本语义单元的匹配上去评测候选摘要和标准摘要之间的相似性，缺少语义方面的维度。因此，如何设计一个合适的评测方法，也是目前文本摘要任务的一个研究方向。



>上面简单介绍了一下摘要任务类型和评价指标。下面，主要介绍一篇发表在EMNLP2019上关于摘要的论文：[《Text Summarization with Pretrained Encoders》](https://arxiv.org/pdf/1908.08345.pdf)，[代码地址](https://github.com/nlpyang/PreSumm)。



该论文主要关注生成式和抽取式摘要上使用统一的BERT变体，展示了预训练语言模型在抽取式和生成式摘要任务中的有效性，提出的模型为进一步提高摘要性能打下了基础。

### 抽取式摘要

首先是在抽取式摘要上的应用，如下图所示：

![1](/img/bertsum/1.png)

抽取式摘要的处理范式是选择性抽取文本中句子作为摘要。这个任务的最大问题是如何获得每个句子向量，然后将句子向量用于二分类。而BERT原生结构只能生成单个句子向量或者句子对的向量。因此作者调整了BERT的输入部分和Embedding来使得它们适应摘要任务。

1. 作者在每个句子的前后都加入“[CLS]”和“[SEP]”的符号，每一个"[CLS]"对应这句话的向量表示。另外，为了区别一个文档中的多个句子，作者使用间隔符号，对于一个句子$sent_i$，作者基于句子的单双编号给他们加一个segment embedding EA/EB，这样对于[$sent_1,sent_2,sent_3...sent_5$]而言，它们的segment embedding就是[EA,EB,EA,EB,EA]。
2. 为了进一步增进句子之间的互动，在BERT之上加了一层transformer的Summarization Layer，只输入每个[CLS]的向量，最后预测当前句子是否保留，进行微调。

### 生成式摘要

在生成式摘要中使用了标准的编码器-解码器框架。编码器是预训练的BERTSUM，解码器是随机初始化的6层Transformer。对于编码器和解码器，分别使用两个$\beta_1=0.9$和$\beta_2=0.999$的Adam优化器。另外，论文提出两阶段微调方法，首先在抽取式摘要任务中对编码器进行微调，然后在生成式摘要中对解码器进行微调，这是由于抽取摘要的目标有利于提高生成式摘要的性能。

### 实验效果

![2](/img/bertsum/2.png)

这里我们只看模型**BERTSUM**在CNN/Daily Mail数据集上的性能表现，从图中可以，不管是生成式摘要还是抽取式摘要，该模型都取得了新的SOTA结果，表明了BERTSUM结构的鲁棒性。

### 感想与展望

从BERT一出现，就在很多NLP任务中展现了其强大的效果。但如何将BERT引入到文本生成任务中，本篇论文提供了一个很好的尝试方式，还融合了生成式和抽取式两个任务，模型结构方面设计很巧妙，不是很复杂，还提供针对BERT长文本输入（限制输入512字符）的解决方法，通过随机初始化长度超过512的位置向量，为想使用BERT进行摘要任务的nlper提供了一个很好方案。

