---
layout:     post
title:      "Word2Vec之Skip-Gram模型"
subtitle:   "Skip-Gram"
date:       2019-03-22 10:00:00
author:     "Wwt"
header-img: "img/skip-gram/bg.jpg"
catalog: true
tags:   
    - Word2Vec
    - Skip-Gram

---

>本文摘自文章《[Word2Vec Tutorial - The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)》，部分内容稍作修改，供学习参考

本文介绍了Word2Vec中的skip-gram神经网络模型。本文的意图是跳过常规的word2vec介绍，对Word2vec进行高度抽象概括，深入了解更多细节。

### 模型

word2vec模型中，主要有skip-gram和CBOW两种模型，从直观上理解，skip-gram是给定输入word来预测上下文，而CBOW是给定上下文，来预测输入词。本文主要介绍Skip-gram模型。实际上，skip-gram模型的基本输入模式是非常简单的，所有小的调整和增强都会使解释变得混乱。

我们从一个高层次的角度开始观察模型，skip-gram模型实际上分为了两个部分，第一部分为建立模型，第二部分是通过模型获取嵌入词向量。word2vec使用了可能在机器学习中都使用的技巧，但实际上我们不会用这个神经网络来完成我们的训练任务。相反，word2vec的目标实际上只是学习隐藏层的权重---我们看到的权重实际上是模型尝试学习的“word2vec”。

另一个你可能已经在无监督学习中学习到的技巧，通过训练的自动编码器(auto-encoder)来压缩隐藏层中的输入向量，继而在输出层将数据解码恢复初始状态。在训练之后，去掉输出层（解压步骤）并只保留隐藏层，这是一个可以学习图像的特性而不需要标记训练数据特性的技巧。

### “假”任务

我们在上面提到，训练模型的真正目的是获得模型基于训练数据学到的隐层权重。为了获得这些权重，我们首先要构建一个完整的神经网络作为我们的“Fake Task”（假任务），稍后我们回过头来再通过“假任务”来间接地得到这些词向量。

我们执行以下操作来训练神经网络。给定句子中间的特定单词（输入单词），查看附近的单词并随机选择一个单词。神经网络将告诉我们词汇表中每个单词成为我们选择的“附近单词”的可能性。例如输入句子“The dog barked at the mailman”，首先我们选句子中间的一个词作为模型的输入词，例如我们选取“dog”作为输入词；有了输入词以后，我们在定义一个叫“窗口大小”的参数，它代表着 我们从当前输入词的一侧（左边或右边）选取词的数量。若窗口大小为2，那么我们最终获得窗口中的词（包括输入词在内），就是['the', 'dog','barked' ,'at']，所以整个窗口的大小为$2*2=4$。另一个参数是“跳跃步数”，它代表着我们从整个窗口中选取多少个不同的词作为我们的输出词，当窗口大小=2，跳跃步数=2时，我们将会得到两组（输出词，输入词）形式的训练数据，即（'dog','barked'），（'dog'，'the'）。

神经网络基于这些训练数据将会输出一个概率分布，这些概率与我们在输入词附近找到每个词作为输出词的可能性。例如，输入到网络中的单词是“Soviet”（苏联），则“Union”和“Russia”这样的单词输出概率远远高于“Watermelon”（西瓜）和“Kangoaroo”（袋鼠）这样的无关单词。因为“Union”和“Russia”在文本中更可能在“Soviet”的窗口中出现。我们将通过给神经网络输入文本中成对的单词来训练它完成上面所说的概率计算。下面的图中给出了一些我们训练样本的例子。我们选定句子“The quick brown foc jumps over lazy dog”，设定我们的窗口大小为2，也就是说我们仅选输入词前后各两个词和输入词进行组合。下图中，蓝色代表input word，方框内代表位于窗口内的单词。

![1](/img/skip-gram/1.png)

模型将会从每对单词出现的次数中学习得到统计结果。例如，神经网络可能会得到更多类似（“Soviet”,“Union”）这样的训练样本对,而对于(“Soviet”,“Sasquatch”)这样的组合却看到的很少。因此，当模型训练完成后，给定一个单词“Soviet”作为一个输入，输出的结果中“Union”或“Russia”要比“Sasquatch”被赋予更高的概率。

### 模型细节

我们如何来表示这些单词呢？首先，我们都知道神经网络只能接受数值输入，我们不能将一个单词作为文本字符串提供给神经网络，所以我们需要一种方法来向网络表示单词。最常用的方法就是基于训练文档来构建一个词汇表在对单词进行独热（one-hot）编码。

假设我们基于训练文档中构成由10000个不重复单词组成的词汇表。我们对这些单词表进行独热编码，得到每个单词都是一个10000维的向量，向量的每个维度的值只有0或1，假设单词“ants”在词汇表中的出现位置为第3个，那么ants的向量就是一个第三维度取值为1，其它位置都为0的10000维的向量。

如在“The dog barked at the mailman”,我们基于这个句子，可以构建一个大小为5的词汇表（忽略大小写和标点符号）（"the","dog","barked","at","the","mailman"）我们对这个词汇表的打次进行编号0-4。那么“dog”就可以被表示为一个5维向量[0,1,0,0,0]。下图是模型结构图：

![2](/img/skip-gram/2.png)

隐藏层神经元上没有激活函数，但是输出层使用了softmax。我们基于成对的单词来训练神经网络，训练样本是（输入词，输出词）这样的单词对，输入词和输出词都是one-hot编码的向量，最后模型的输出是一个概率分布。

### 隐层

介绍完单词的编码和训练样本的选取，接下来我们来看下模型的隐层。如果我们现在想用300个特征来表示一个单词（即每个词可以表示为300维的向量），那么隐层的权重矩阵应该为10000行，300列（隐层有300个节点）。

> Google在最新发布的google news数据集训练的模型使用的就是300个特征的词向量。词向量的维度是一个可以调节的超参数（在Python的gensim包中封装的Word2Vec接口默认的词向量大小为100， window_size为5）。

看下面的图片，左右两张图分别从不同角度代表了输入层-隐层的权重矩阵。左图中每一列代表一个10000维的词向量和隐层单个神经元连接的权重向量。从右边的图来看，每一行实际上代表了每个单词的词向量。

![3](/img/skip-gram/3.png)

所以，这一切的最终目标实际上只是学习这个隐藏层的权重矩阵，当模型训练结束时我们就会丢弃输出层。

我们现在回来接着通过模型定义来训练我们的这个模型。

上面本文提到，输入词和输出词都会被进行one-hot编码。仔细想一下，我们的输入被one-hot编码以后大多数维度上都是0（实际上仅有一个位置为1），所以这个向量相当稀疏，那么会造成什么结果呢？如果我们将一个1 x 10000的向量和10000 x 300的矩阵相乘，它会消耗相当大的计算资源，为了高效计算，它仅仅会选择矩阵中对应的向量中维度值为1的索引行（这句话很绕），看下图就明白。

![4](/img/skip-gram/4.png)

我们来看一下上图中的矩阵运算，左边分别是1 x 5和5 x 3的矩阵，结果应该是1 x 3的矩阵，按照矩阵乘法的规则，结果的第一行第一列元素为0 x 17 + 0 x 23 + 0 x 4 + 1 x 10 + 0 x 11 = 10，同理可得其余两个元素为12，19。如果10000个维度的矩阵采用这样的计算方式是十分低效的。

为了有效地进行计算，这种稀疏状态下不会进行矩阵乘法计算，可以看到矩阵的计算的结果实际上是矩阵对应的向量中值为1的索引，上面的例子中，左边向量中取值为1的对应维度为3（下标从0开始），那么计算结果就是矩阵的第3行（下标从0开始）—— [10, 12, 19]，这样模型中的隐层权重矩阵便成了一个”查找表“（lookup table），进行矩阵计算时，直接去查输入向量中取值为1的维度下对应的那些权重值。隐层的输出就是每个输入单词的“嵌入词向量”。

### 输出层

经过神经网络隐层的计算，ants这个词会从一个1 x 10000的向量变成1 x 300的向量，再被馈送到到输出层。输出层是一个softmax回归分类器，它的每个结点将会输出一个0-1之间的值（概率），这些所有输出层神经元的结点概率之和为1。

下面是一个例子，训练样本为 (input word: “ants”， output word: “car”) 的计算示意图

![5](/img/skip-gram/5.png)

>值得注意是，神经网络对输出词相对于输入词的偏移量一无所知。 它在输入之前与之后的单词之间没有学习不同的概率集。 为了理解其含义，我们可以认为在训练语料库中，“York”一词的每一次出现都以“New”一词开头。 也就是说，根据训练数据，'New'在'York'附近的概率为至少为100％。 但是，如果我们在'York'附近取10个单词并随机选择其中一个，它'为“New'”的概率不是100％; 你可能已经选择了附近的其它一个词。

### 理解

如果两个不同的单词具有非常相似的“上下文”（即它们周围可能出现的单词），那么模型需要为这两个单词输出非常接近的结果。 如果两个单词具有相似的上下文，那么模型就会为这两个单词学习到相似的向量！

两个词有类似背景的意思是什么？ 你可以认为像“intelligent”和“smart”这样的同义词具有非常相似的背景。 或者那些相关的词，如“engine”和“transmission”，也可能具有类似的背景。

实际上，这种方法实际上也可以帮助你进行词干化（stemming），例如，神经网络对”ant“和”ants”两个单词会习得相似的词向量。

### 参考

>[Word2Vec Tutorial - The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)
>
>[一文详解 Word2vec 之 Skip-Gram 模型（结构篇）](https://www.leiphone.com/news/201706/PamWKpfRFEI42McI.html)