---
layout:     post
title:      关系抽取新方案--TPLinker
subtitle:   "TPLinker"
date:       2022-03-12 10:00:00
author:     "Wwt"
header-img: "img/tplinker/bg.png"
catalog: true
tags:   
    - NLP
---

### 引言

近年来，从非结构化中提取实体和关系引起了越来越多的关注，但仍然具有挑战性，因为识别与共享实体的重叠关系具有内在的困难。以前的研究表明，联合学习可以带来显著的性能提升。然而，它们通常涉及顺序相关的步骤，存在曝光偏差的问题。在训练时，它们利用真实条件进行预测，而在推理时则必须从头开始提取，这种差异会导致错误累积。为了缓解这个问题，我们在本文中提出了一种单阶段联合提取模型，即TPLinker，它能发现共享一个或两个实体的重叠关系，同时不受曝光偏差的影响。TPLinker将联合提取归结为标记对链接问题，并引入一种新的握手标记方案，该方案将实体对的边界标记对齐在每种关系类型下。实验结果表明，TPLinker在重叠和多关系抽取方面表现明显更好，并在两个公共数据集上达到了最先进的性能。

### 1.介绍

从非结构化文本中提取液实体和关系是自动知识库构建的关键步骤。传统的流水线方法首先抽取实体提及、然后对候选实体对之间的关系类型进行分类。但是，由于实体检测和关系分类完全分离，这些模型忽略了两个子任务之间的相互作用和相关性，容易产生级联错误。

在过去的几年里，建立联合模型同时抽取实体和关系的研究越来越受到关注。最近的研究表明，联合学习可以有效地整和实体和关系的信息，因此在两个子任务中都取得了更好的性能。    Zheng et al （2017）提出了一种统一的标记方案，将联合提取转化为序列标记问题，但缺乏识别重叠关系的优雅性，即一个实体文本可以出现在不同关系中。

![1.png](/img/tplinker/1.png)

虽然这些方法都取得了不错的效果，但是都有同样的**曝光偏差**问题，对于基于解码器的方法，在训练时groud truth token作为上下文，而在推理时，整个序列又模型自己生成结果。因此，训练和推理时的预测token来自不同的分布，即来自数据分布而不是模型分布。同样的，基于分解的方法在训练过程中使用gold主实体作为特定输入引导模型提取客体和关系，而在推理过程中，输入实体由训练过的模型给出，导致训练与推理之间存在差距。

在本文中，我们提出了一种用于实体和重叠关系联合提取的一阶段方法，即TPLinker，它弥合了训练和推理之间的鸿沟。TPLinker将联合提取任务转换为**T**oekn  **P**air **L**inking链接问题。

给定一个句子，两个位置p1,p2和一个特定关系r，TPLinker回答三个YES NO问题。

1. p1和p2 是否分别是同一个实体的起始位置和结束位置？

2. p1和p2是否为别为两个具有r关系实体的起始位置？

3. p1和p2是否分别是r关系的两个实体的末端位置？

### 2.策略

我们设计了一个握手标记方案，为每个关系标注三个Token Link矩阵来回答上述三个问题。然后使用这些链接矩阵来解码不同的标注结果，从中我们可以提取所有的实体及其重叠关系。

#### 标注

文章提出了三种链接方式，**紫色**代表两实体各自内部的头尾连接，**红色**代表两实体头连接，**蓝色**代表两实体的尾连接。同一种颜色的连接标记，会被表示在同一个矩阵。

![2.png](/img/tplinker/2.png)

- 其中紫色标签代表实体的头尾关系，红色标签代表两个实体的头部关系，蓝色标签代表两个实体的尾部关系，因为三种关系重叠，所以三种标签是存在与不同的矩阵。

- 因为实体尾部不可能出现在头部之前，所以我们可以舍弃掉下三角区域，但是红标和蓝标可能出现在下三角区域，因此，我们可以把下三角区域的值映射到上三角，并标记为2，如上图右边所示。

这样做之后，它不再是一个完整的矩阵，在实际操作之中，我们把剩余的项平摊成一个序列，如下图所示：

![3.png](/img/tplinker/3.png)

为了方便计算，用一个映射来记住原始矩阵中的位置。这个序列就像所有标记的握手，这就是为什么我们把这个方案成为握手标记方案的原因。

这个标记方案可以自然地处理单点重叠问题和嵌套实体问题。在本例中"New York City"和"NewYork"是嵌套的，并且共享一个对象"De Blasio"，这对以前的许多方法都是一个挑战性的问题。

总的来说，如上图所示，将联合提取任务分解为$2N+1$序列标记子任务，其中$N$表示预定义关系类型的的数量，每个子任务构建一个长度为$\frac{n(n+1)}{2}$的标签序列，其中$N$是输入语句的长度。我们的标记方案似乎是非常低效的，因为标记序列的长度随着句子长度的增加呈平方数增加。幸运的是，实验结果表明，利用编码器顶部的轻量级标记模型，TPLinker比起目前的SOTA很有竞争力。因为编码被所有标记共享，并且只需一次产生$n$个token的表示。

#### 解码

在上述介绍中，(“New”,“York”)，("New","City")和("De","Blasio")在属性中被标记为EH-ET序列，意思是"New York","New York City"和"De Blasio"是三个实体。

对于关系"mayor"，（"New","De"）在SH-to-OH序列中被标记为1，这意味着以"New"开始的主语就是以"De"开始的对象。("City","Blasio")在ST-to-OT序列中被标记为1，这意味着主体和客体都是以"City"和“Blasio”的实体结尾。根据这三个序列所代表的信息，可以解码出一个三元组(“New York”，“Mayor”，“Blasio”)。

![4.png](/img/tplinker/4.png)

上面算法解释了解码的过程。具体如下：

- 解码EH-to-ET可以得到句子中所有的实体，用实体**头token idx 作为key**，实体作为value，存入字典D中；

- 对每种关系r[实体关系R是事先定义好的一个集合]，解码ST-to-OT得到token对**存入集合E**中，解码SH-to-OH得到token对并在D中关联其token idx 的实体value;

- 对上一步中得到的SH-to-OH token对的所有实体value对，在集合E中依次查询是否其尾token对在E中，进而得到三元组信息。

#### 实体对表示

给出一个句子，我们首先通过一个基本的编码器将每个token映射到一个低维的上下文向量$hi$，然后我们可以为token pair $(w_i,w_j)$生成一个表示$h_i,h_j$，如下

$$
h_{i,j}=tanh(W_h[h_i;h_j]+b_h),j \geq i
$$

#### 握手标注

对于EH-ET,SH-OH,ST-OT,我们使用统一的标注框架。

$$
P(y_{i,j})=Softmax(W_o h_{i,j}+b_o) \\
link(w_i,w_j) =argmaxP(y_{i,j}==l)
$$

### 3.实验结果

该模型在NYT和WebNLG两个关系抽取任务上都达到了当时的SOTA性能。

![5.png](/img/tplinker/5.png)

### 4.未来的工作

这里主要提一下值得改进的地方：

1. 论文中token对的向量表示采用的是直接拼接，这种简单的方式可能并不能展现最佳的性能。

2. 实体和关系的识别使用的都是相同的向量表达，这可能会相互干扰。

3. 模型将原本长度为$N$ 的序列扩展成了$O(N_2)$的序列，这无疑增加了开销，使得处理长文本变得比较昂贵。另外，矩阵的稀疏性和标签的极度不平衡对性能有一定的影响。

### 参考

> [论文翻译 PLinker: Single-stage Joint Extraction of Entities and Relations
> Through Token Pair Linking](https://blog.csdn.net/li_jiaoyang/article/details/111315300)
> 
> [关系重叠？实体嵌套？曝光偏差？这个模型统统都搞得定！](https://mp.weixin.qq.com/s/T_bSjHz8XEeVQMBKVKOH0Q)
