---
layout:     post
title:      "贝叶斯分类器"
subtitle:   "贝叶斯"
date:       2017-03-04 11:00:00
author:     "Wwt"
header-img: "img/bg/beiyesi.jpg"
catalog: true
tags:	
    - 贝叶斯分类
---

​	贝叶斯分类是一类分类算法的总称，这类算法均以贝叶斯定理为基础，故统称为贝叶斯分类。

## 介绍

​	贝叶斯定理之所以有用。是因为我们生活中经常遇到这种情况：我们可以很容易直接得出 $P(A\mid B)$，$P(B\mid A)$ 则很难直接得出，但我们更关心 $p(B\mid A)$ ,贝叶斯定理就为我们打通从 $P(A\mid B)$ 获得 $P(B\mid A)$ 的道路。贝叶斯定理如下：

​			$$P(B\mid A)=\frac{P(A\mid B)P(B)}{P(A)}$$ （1）

​	其中，$P(B)$是先验概率；$P(A\mid B)$ 是样本A相对于类标记B的类条件概率，或称为“似然”(likelihood);p(A)是用于归一化的“证据因子”。对给定样本A,证据因子$P(A)$与类标记无关，因此估计 $P(B\mid A)$ 的问题就转化为如何基于训练数据D来估计先验$P(B)$和似然 $P(A\mid B)$ .

​	类先验概率$P(B)$表达了样本空间中各类样本所占的比例，根据大数定律，当训练集包含充足的独立同分布样本时，$P(B)$可通过各类样本出现的频率来进行估计。

​	对类条件概率 $P(A\mid B)$ 来说，由于它涉及关于A所以属性的联合概率，直接根据样本出现的频率来估计将会遇到严重的困难，例如，假设样本的d个属性都是二值的，则样本空间将有 $2^d$ 种可能取值。在现实应用中，这个值往往大于训练样本数m，也就是说，很多样本取值在训练集中根本没出现，直接使用频率来估计 $P(A\mid B)$ 显然不可行，因为“未被观测到”与“出现概率为零”通常是不同的。

## 极大似然估计

​	估计类条件概率是一种常用策略是先假定其具有某种确定的概率分布形式，再基于训练样本对概率分布的参数进行估计。具体地，记关于类别c的类条件概率为 $P(x\mid c)$ ,假设 $P(x\mid c)$ 具有确定的形式并且被参数向量$θ_c$唯一确定，则我们的任务就是利用训练集D估计参数$θ_c$

​	事实上，概率模型的训练过程就是参数估计过程，对于参数估计，统计学界的两个学派分别提供了不同的解决方案：频率主义学派认为参数虽然未知，但却是客观存在的固定值，因此，可通过优化似然函数等准则来确定参数值；贝叶斯学派则认为参数是未观察到的随机变量，其本身也可由分布，因此，可假定参数服从一个先验分布，然后基于观测到的数据来计算参数的后验分布。

​	**原理**

>给定一个概率分布$D$,已知其概率密度函数(连续分布)或概率质量函数(离散分布)为$f_D$，以及一个分布参数$θ$，我们可以从这个分布中抽取一个具有n个值的采样$X_1,X_2,···,X_n$，利用$f_D$计算出其概率：
>
>$$P(x_1,x_2,···,x_n)=f_D(x_1,····,x_n \mid θ)$$
>
>但是，我们 不可能知道$θ$的值，尽管我们知道这些采样数据来自于分布D。那么我们如何才能估计出$θ$呢？一个自然的想法是从这个分布中抽出一个具有n个值的采样$X_1,X_2,···,X_n$，然后用这些采样数据来估计θ,一旦我们获得$X_1,X_2,···,X_n$,我们就能求得一个关于$θ$的估计。最大似然估计会寻找关于$θ$的最可能的值(即，在所有可能的$θ$值中，寻找一个值使这个采样的“可能性”最大化)。这种方法正好同一些其他的估计方法不同，如$θ$的非偏估计，非偏估计未必会输出一个最可能的值，而是输出一个既不高也不低估的$θ$值。
>
>要在数学上实现最大似然估计法，我们首先要定义似然函数：
>
>lik(θ)=f_D(x_1···x_n \mid θ)
>
>并且在$θ$的所有取值上通过令一阶导数为零，使这个函数取最大值。这个使可能性最大的$hat θ$值即称为$θ$的最大似然估计。
>
>注意：
>
>- 这里的似然函数是指$x_1,···x_n$不变时，关于$θ$的一个函数。
>- 最大似然估计函数不一定是唯一的，甚至不一定存在。



令$D_c$表示训练集$D$中第$c$类样本组成的集合，假设这些样本是独立同分布的，则参数$θ_c$对于数据集$D_c$的似然是

​			$$P(D_c \mid θ_c)= \Pi_{x \in {D_c}} P(x  \mid θ_c)$$

​	对$θ_c$进行极大似然估计，就是去寻找最大化似然$P(D_c \mid θ_c)$的参数值$\hat θ_c$。直观上看，极大似然估计是视图在$θ_c$所有可能的取值中，找到一个能使数据出现的“可能性”最大的值。

​	上式中的连乘操作易造成下溢，通常使用对数似然

​	$$\begin{align} LL(θ_c)&=log P(D_c \mid θ_c) \\ &=\sum _{x \in D_c} log P(x \mid θ_c)\end{align}$$

​	此时参数$θ_c$的极大似然估计$\hat θ_c$为

​		$$\hat θ_c=argmax _{θ_c} LL(θ_c)$$

​	例如，在连续属性情形下，假设概率密度函数$P(x\mid c) ~ N(\upsilon_c, \sigma^2)$，则参数$\upsilon_c$和$\sigma^2)$的极大似然估计为

​	$$\begin{align} \hat\upsilon_c&=\frac{1}{\mid D_c \mid}\sum _{x\in D_{c}} x\\ \hat\sigma^2&=\frac{1}{D_c}\sum _{x\in D_c} (x-\hat \upsilon)(x-\hat\upsilon)^T\end{align}$$

​	也就是说，通过极大似然法得到的正态分布均值就是样本均值，方差就是$(x-\hat \upsilon)(x-\hat\upsilon)^T$的均值，这显然是一个符合直觉的结果，在离散属性情形下，也可通过类似的方式估计类条件概率。

​	需要注意的是，这种参数化的方法虽能使类条件概率估计变得相对简单，但估计结果的准确性严重依赖于假设的概率分布形式是否符合潜在的真实数据分布。在现实应用中，欲做出能较好地接近潜在真实分布的假设，往往需在一定程度上利用关于应用任务本身的经验知识，若则仅凭“猜测”来假设概率分布形式，很可能产生误导性的结果。

## 朴素贝叶斯分类器

​	不难发现,基于贝叶斯公式(1)来估计后验概率 $P(B\mid A)$ 的主要困难在于；类条件概率 $P(A\mid B)$ 是所有属性上的联合概率，难以从有限的训练样本直接估计而得。为避开这个障碍，朴素贝叶斯分类器采用了“属性条件独立性假设”：对已知类别，假设所以属性相互独立。换言之，假设每个属性独立的对分类结果发生影响。

​	基于属性条件独立性假设，式(1)可重写为

​	$$P(c\mid x)=\frac{P(x\mid c)P(c)}{P(x)}=\frac{P(c)}{P(x)}\prod_{i=1}^d{P(x_i\mid c)}$$  （2）

其中d为属性数目 $ _i$ 为X在第i个属性上的取值。

​	由于对所有类别来说P(x)相同，因此朴素贝叶斯分类器的表达式为

​	$$h_{nb} (x)=argmax P(c)\prod_{i=1}^{d}P(x_i\mid c)$$  （3）

​	显然，朴素贝叶斯分类器的训练过程就是基于训练集D来估计先验概率P(c)，并为每个属性估计条件概率 $P(x_i\mid c)$ 。

​	令 $D_c$ 表示训练集D中第c类样本组成的组合，若有充足的独立同分布样本，则可以容易地估计出类先验概率

​	$$P(c)=\frac{D_c}{D}$$  （4）

​	对离散属性而言，而 $D_cx_i$ 表示$D_c$中在第i个属性上取值$x_i$的样本组成的集合，则条件概率 $P(x_i\mid c)$ 则可估计为

​	$$P(x_i\mid c)=\frac{D_{c,xi}}{D_c}$$ .  （5）

​	下面我们用西瓜数据集3.0训练一个朴素贝叶斯分类器，对测试例“测1”进行分类。

| 编号   | 色泽   | 根蒂   | 敲声   | 纹理   | 脐部   | 触感   | 密度    | 含糖率   | 好瓜   |
| ---- | ---- | :--- | ---- | ---- | ---- | ---- | ----- | ----- | ---- |
| 测1   | 青绿   | 蜷缩   | 浊响   | 清晰   | 凹陷   | 硬滑   | 0.697 | 0.460 | ？    |

​	首先估计类先验概率P(c)，显然有

​	P(好瓜=是)= $\frac{8}{17}$

​	然后为每个属性估计条件概率 $P(x_i\mid c)$ :

​	$$p_{青绿\mid 是}=P(色泽=青绿\mid 好瓜=是)=\frac{3}{8}$$

​	$$p_{青绿\mid 否}=P(色泽=青绿\mid 好瓜=否)=\frac{3}{9}$$

​	$$p_{蜷缩\mid 是}=P(根蒂=蜷缩\mid 好瓜=是)=\frac{5}{8}$$

​	$$p_{蜷缩\mid 否}=P(根蒂=蜷缩\mid 好瓜=否)=\frac{3}{9}$$

​	$$p_{浊响\mid 是}=P(敲声=浊响\mid 好瓜=是)=\frac{6}{8}$$

​	$$p_{浊响\mid 否}=P(敲声=浊响\mid 好瓜=否)=\frac{4}{9}$$

​	$$p_{清晰\mid 是}=P(纹理=清晰\mid 好瓜=是)=\frac{7}{8}$$

​	$$p_{清晰\mid 否}=P(纹理=清晰\mid 好瓜=否)=\frac{2}{9}$$

​	$$p_{凹陷\mid 是}=P(1脐部=凹陷\mid 好瓜=是)=\frac{6}{8}$$

​	$$p_{凹陷\mid 否}=P(脐部=凹陷\mid 好瓜=否)=\frac{2}{9}$$

​	$$p_{硬滑\mid 是}=P(触感=硬滑\mid 好瓜=是)=\frac{6}{8}$$

​	$$p_{硬滑\mid 否}=P(触感=硬滑\mid 好瓜=否)=\frac{6}{9}$$

​	$$p_{密度:0.697\mid 是}=P(密度=0.697\mid 好瓜=是)=1.959$$

​	$$p_{密度:0.697\mid 否}=P(密度=0.697\mid 好瓜=否)=1.203$$

​	$$p_{含糖:0.460\mid 是}=P(含糖=0.460\mid 好瓜=是)=0.788$$

​	$$p_{含糖:0.460\mid 否}=P(含糖=0.460\mid 好瓜=否)=0.066$$

 于是，有

$$P(好瓜=是)×P_{青绿\mid 是}×P_{蜷缩\mid 是}×P_{浊响\mid 是}×P_{清晰\mid 是}×P_{凹陷\mid 是}×P_{硬滑\mid 是}×P_{密度:0.697\mid 是}×P_{含糖:0.460\mid 是}=0.038$$

$$P(好瓜=否)×P_{青绿\mid 否}×P_{蜷缩\mid 否}×P_{浊响\mid 否}×P_{清晰\mid 否}×P_{凹陷\mid 否}×P_{硬滑\mid 否}×P_{密度:0.697\mid 否}×P_{含糖:0.460\mid 否}=0.68×10^{-5}$$

由于0.038> $6.80×10^{-5}$ ,因此，朴素贝叶斯分类器将测试样本“测1”判别为“好瓜”。

​	需注意，若某个属性在训练集中没有与某个类同时出现过，则直接基于式（5）进行概率估计，再根据式（3）进行判别将出现问题。例如，在使用西瓜数据集3.0训练朴素贝叶斯分类器时，对一个“敲声=清脆”的测视例，有

​	$$P_{清脆\mid 是}=P(敲声=清脆\mid 好瓜=是)=\frac{0}{8}=0$$

​	由于式(3)的连乘式计算出的概率为零，因此，无论该样本的其他的属性是什么，哪怕在其他属性上明显像好瓜，分类的结果都将是“好瓜=否”，这显然不太合理。

​	为了避免其他属性携带的信息被训练集中未出现的属性值“抹去”吗，在估计概率值时通常要进行“平滑”处理，常用“拉普拉斯修正”。具体来说，令N表示训练集D中可能的类别数，$N_i$表示第i个属性可能的取值数，则式(4)和式(5)分别修正为

​	$$P(c)=\frac{D_c+1}{D+N}$$

​	$$P(x_i\mid c)=\frac{D_{c,x_i}+1}{D_c+N_i}$$

​	例如再上述例子中，类先验概率可估计为

​	$$P(好瓜=是)=\frac{8+1}{17+2}=0.474$$

​	类似地, $P_{青绿\mid 是}$ 可估计为

​	$$P_{青绿\mid 是}=P(色泽=青绿\mid 好瓜)=\frac{3+1}{8+3}=0.364$$.

​	显然，拉普拉修斯修正避免了因训练集样本不充分而导致概率估值为零的问题，并且在训练集变大时，修正过程所引入的鲜艳的影响也会逐渐变得可忽略，使得估值渐趋于实际概率值。

## 	参考作者

​	[算法杂货铺----分类算法之朴素贝叶斯分类](http://www.cnblogs.com/leoo2sk/archive/2010/09/17/naive-bayesian-classifier.html)

​	[机器学习]()