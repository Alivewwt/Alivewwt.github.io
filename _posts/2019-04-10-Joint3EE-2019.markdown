---
layout:     post
title:      "一个模型解决实体、触发词和论元抽取任务"
subtitle:   "one for All"
date:       2019-04-10 10:00:00
author:     "Wwt"
header-img: "img/Joint3EE/bg.png"
catalog: true
tags:   
    - 联合模型
    - 事件抽取
	- NLP
---

先前的事件抽取工作中假设实体已经人工标注好，主要集中在触发词和论元抽取两个任务中。实体信息通常由命名实体工具抽取出往往是不现实的，这会导致错误信息传递到触发词抽取和论元抽取任务中。只有少数工作将实体识别、触发词抽取和论元抽取三个任务联合起来，只使用一个模型抽取出三者的信息。但是，上述工作采用离散的特征工程表示单任务和各任务之间交互关系的上下文信息具有局限性。因此，在本文中我们提出一个通过共享隐层表示的深度联合模型同时抽取实体、触发词和论元。实验结果表明，本文提出的模型在事件抽取任务中取得了最先进的性能。

>本文发表在AAAI2019，是一个联合框架对实体、触发词和论元进行抽取。

### 问题描述

事件抽取作为信息抽取中的一个重要的NLP任务。我们采用ACE 2005的定义：事件是由句子中的触发词激活，且与实体构成了事件中不同的语义角色关系。

事件抽取是一个具有挑战性的任务，根据事件的定义可知，它是由三个子任务组成的。特别地，第一个任务是识别出句子中的实体（Entity Mention Detection-EMD），第二个任务是要识别出句子中的触发词并对其分类（Event Detection-ED），最后，在第三个任务中是要识别出触发词和实体的具体关系，并正确分类出实体在事件充当的角色（Argument Role Prediction-ARP）。流线型模型的处理子任务顺序是"EMD-ED-ARP"。考虑下面ACE 2005语料中的一个例子：

>Another a-10 warthog was hit today.

在上面提到的句子中，EMD系统需要识别出"a-10 warthog"是一个实体，实体类型是"vehicle"，另外"today"是一个时间表达式。对于ED系统来说，应当识别出"hit"是触发词，且触发了"Attack"事件类型。最后，ARP系统应该识别出"a-10 warthog"在"Attack"事件中充当了"target"类型的角色，另外，"today"是事件发生的时间。

之前的大部分事件抽取工作采取了简化方法，只关注其中一个或者两个任务，要么使用标注好的实体信息，要么完全忽略实体信息。其中一个重要问题就是会导致错误传递，上一个任务出现的错误会继续向下一个任务进行传递，造成后来任务的性能低下。另外，事件抽取中的流线型模型没有任何机制来捕获各子任务间的依赖和交互性，以便后一个任务可以提高前一个任务的决策正确性。另一个方面，前面的子任务只能通过离散输出与后面的任务进行通信，无法传播更深层次的信息到后面的任务中。考虑一个事件抽取系统，EMD模块与ED和ARP模块独立开来，EMD模块单独执行，那么ED和APR模块则无法更正EMD模块中的错误。同时，EMD模块只能提供给ED和ARP模块具有边界的实体以及实体类型，像那些隐藏的上下文等深层信息或更细粒度的语义实体信息不能传递给ED和ARP模块中。这会使得信息在各子任务中传递效率低下，导致事件抽取性能下降。

为了解决上述问题，我们提出了一个深度学习的单模型来联合抽取实体、触发词和论元。特别地，我们使用了一个双向的循环神经网络（RNN）来学习句子中词的隐藏表示，来执行EMD、ED和ARP三个子任务。一方面，双向RNN能够学习到通过实值表示的隐藏结构信息，并且缓解那些硬匹配的二进制特征问题。另一方面，三个任务共享隐藏表示，使得任务之间知识共享，捕获任务之间的依赖和交互关系，从而提高事件抽取的性能。

### 模型介绍

我们从句子级别提出了一个联合模型来执行实体、触发词和论元抽取三个任务。首先， 通过$W=w_1,w_2,…,w_n$表示一个句子的所有词。为了解决EMD问题，我们将它看成一个序列标注问题，为每个词分配一个标签。标签序列$$E=e_1,e_2,…,e_n$$表示了句子$W$实体提及的边界和类型，我们使用BIO模式来生成句子序列中每个词的BIO标签。

对于触发词抽取任务，我们跟随前人的工作，假设句子中的触发词只是单个词，这基本就是一个单词分类问题，对句子中的每个词$w_i \in W$,我们需要预测事件类型$t_i$($t_i$可以是"other",表明该候选触发词没有触发任何事件)。

对于论元抽取任务，我们需要识别出实体在事件中充当的角色。基本上，我们需要为每对触发词和实体预测其论元角色。我们开始选择标记实体提及的索引位置，构成每个触发词-实体对。

事件抽取任务的结构包含五个部分：句子编码，句子表示，实体抽取，触发词抽取和论元抽取，这五个部分按照线性排列顺序（从左到右），前两个部分是为了将输入句子$W$转换成隐藏表示，后面的三个部分使用这隐藏表示来执行EMD、ED和ARP三个任务。模型结构如下图所示：

![1](/img/Joint3EE/1.png)

#### 句子编码

在句子编码模块，每个$w_i \in W$转换成向量$x_i$，通过拼接下面的向量：

+ 预训练的词向量，并且在训练过程中更新词向量；
+ 使用二进制向量表示每个$w_i \in W $的词性（pos），chunk和依存关系。特别地，我们对句子使用了词性标注工具、chunker和依存句法分析器，得到了词性标签，chunking 标签（BIO模式）以及句法树中依存关系。最后，我们生成独热向量来表示词性标签和chunking 标签，同样地，我们使用二进制向量表示句法树的$w_i$的依存关系。

#### 句子表示

经过句子编码阶段后，输入句子$W$表示为$X=x_1,x_2,…,x_n$.在句子表示阶段，序列向量$X$送入到双向RNN神经网络中，在这里，我们使用GRU来实现RNN模型，获得了隐藏层向量表示$h_1,h_2,…,h_n$，来编码整个序列的上下文信息。特别地，我们使用隐层向量$H$作为共享表示来执行EMD、ED和ARP三个任务，这能保证知识在三个任务中进行转移。

针对事件抽取任务，我们对$W$进行解码，目标是为了联合预测E，T和A的标签，最大化三个预测任务的联合概率：


$$
\begin{align}P(A,T,E\mid W) &= P(E \mid W)*P(A,T\mid E,W)\\
&=P(E\mid W)*P(a_1,t_1\mid E,W)\\
&*P(a_2,t_2\mid E,W,a<2,t<2)\\
&...\\
&*P(a_n,t_n\mid E,W,a<n,t<n)
\end{align}
$$


其中$a_i$表示论元角色矩阵$A$中的第$i$行角色标签。

基于上述分解，我们首先预测句子中每个词的实体类型（即计算EMD中的$P(E \mid W)$），然后，我们从左向右扫描句子的每个词，计算概率$P(a_i,t_i\mid E,W,a<i,t<i)$来预测触发词和论元。需要注意的是，对$P(a_i,t_i \mid E,W,a<i,t<i)$建模能够使用来自$a<i$和$t<i$的信息来揭示出现在句子中的多个事件之间的相互依赖关系，从而可以更好地预测触发词和论元。

本文中，我们将参考局部信息$D_i$来表示句子中的块$w_i \in W$与词向量拼接在一起作为串联向量。

#### 实体抽取

我们将实体类型检测的目标函数展开如下：


$$
P(E\mid W)=P(e_1\mid W)P(e_2\mid W,e<2)...P(e_n\mid W,e<n)
$$


我们的目标是最大化这个概率，其中


$$
P(e_i\mid W,e,i)= FF^{EMD}(R_i^{EMD})
$$


其中：


$$
\begin{gather}
R^{EMD}_i=[h_i,D_i]\\
FF^{EMD},全连接层\\
e<t=e_1,e_2,...,e_{i-1}\\
D_i=[d_{i-u},...,d_i,...,d_{i+u}]
\end{gather}
$$

#### 触发词和论元抽取

我们将事件触发词抽取和论元抽取展开成如下目标函数，我们的目标是最大化这个目标函数：
$$
\begin{align}
P(a_i,t_i\mid E,W,a<i,t<i)&=P(t_i\mid E,W,a<i,t<i)\\
&*P(a_{i1}\mid E,W,a<i,t<{i+1})\\
&*P(a_{i2}\mid E,W,a_i,<2,t<i+1)\\
&...\\
&*P(a_{in}\mid E,W,a_i,<n,a<i,t<i+1)
\end{align}
$$


其中:


$$
\begin{align}
a_i<j&=a_i,1,a_i,2,…,a_i,j-1\\
P(t_i\mid E,W,a<i,t<i)&=FF^{ED}(R_{ED}^i)\\
P(a_{ij} \mid E,W,a_{i},<j,a<i,t<i+1)&=FF^{ARP}(R^{ARP}_{ij})\\
R^{ED}_i&=[h_i,D_i]\\
R^{ARP}_{ij}&=[h_i,D_i,h_j,D_j,V(e^p_i),V(t^p_j),M_i,B_{ij}]
\end{align}
$$


其中：

+ $e_i$:第$i$个词的实体类型
+ $t_j$: 第$j$个词的触发词类型
+ $B_{ij}$:第$i$个词和第$j$个词的最短依赖树信息
+ $M_i$:第$i$个词之前出现的事件类型和论元语义角色
+ $V(x)$是将标签$x$转换成独热向量来表示标签的函数

#### 训练

我们训练联合模型通过优化负对数似然函数$$C(A,T,E,W)=-logP(A,T,E|W)$$,为了将三个任务EMD、ED和ARP的损失函数汇合在一起，我们将损失函数定义成以下形式：
$$
\begin{align}
C^*(A,T,E,W)&= -\alpha \sum^n_{i=1}logP(e_i\mid W,e<i)\\
&-\beta \sum^n_{i=1}logP(t_i\mid E,W,a<i,t<i)\\
&-\gamma \sum^n_{i=1} \sum^n_{j=1}logP(a_{ij}\mid E,W,a_i,<j,a<i,t<{j+1})
\end{align}
$$


其中，$\alpha,\beta,\gamma$是超参数，使用随机梯度下降算法来优化损失函数。

### 实验结果

#### 事件抽取结果

本文在ACE 2005的英文语料库上进行对比实验得到以下结果，

![2](/img/Joint3EE/2.png)

从表中可以看出，本文提出的联合训练模型在触发词识别和分类以及论元识别和分类的任务中都达到了最优的F1值。

#### 实体抽取结果

![3](/img/Joint3EE/3.png)

实验结果证明，本文提出的模型在实体类型检测上的F1值同样达到了最优。

#### 错误分析

为了分析本文提出的模型在ED任务上的作用，我们对错误进行统计，从表1中可以看出触发词识别和分类的错误情况分别为72.5%和69.8%，效果差不多，可见主要的错误来源于未能准确判断一个词是否属于trigger word。而通过对未能检测出来的trigger word 的研究发现主要是训练集中未出现trigger,另外，通过对识别出的触发词进行分类研究发现，主要错误来源于该词附近出现了优误导性的上下文信息，而本文的模型不能很好地判断，比如，下面句子中的触发词"fire"可能会由于car的出现而被错误判断为"Attack"事件类型，这启发我们去研究一个更好的能给encode上下文模型：

>…also take over GE's US car and fire insurance operations,the reports said.

对于论元抽取任务，我们发现大部分的论元正确识别，但是论元角色分类错误，通过统计，总共有209个论元，其中有23.9%的论元角色分类正确，但是事件类型分配错误，剩下的159个论元（76.1%）错误分类，还有15.1%的论元实体类型分配错误。所以，主要的问题是模型不能正确判断出不同的论元角色，论元角色之间具有歧义，如place 和 destination,origin和destination,seller和buyer。识别出这些论元角色之间的区别需要更好的机制或网络结构对输入句子进行建模。

### 参考

>[《one for All:Neural Joint Modeling of Entities and Events》](https:arxiv.org/pdf/1812.00195.pdf)

