---
layout:     post
title:      "理解LSTM神经网络"
subtitle:   "一步一步理解LSTM"
date:       2017-10-24 11:00:00
author:     "Wwt"
header-img: "img/LSTM/bg.png"
catalog: true
tags:   
    - LSTM
    - 深度学习
---

>译自《Understanding LSTM Networks》，作者 colah

### 循环神经网络

人类并不是每时每刻都从头开始思考。在你阅读这篇文章的时候，你都是基于自己已经拥有的先前知识来推断当前词的真实含义。我们不会抛弃所有东西，然后重新开始思考。即我们的思想用于持久性。

传统的神经的网络不能做到这一点，这看起来是一个巨大的弊端。例如，假设你希望对电影中每个时间点正在发生的事件进行分类	。传统的神经网络很难使用电影中先前的事件推断后续发生的事件。

RNN解决了这个问题。RNN是包含循环的网络，允许信息的持久化。

![1](/img/LSTM/1.png)

在上面的示例图中，神经网络的模块A正在读取某个输入$x_i$，并输出一个值$h_i$。循环可以使得信息可以从当前步传递到下一步。

这些循环使得RNN看起来非常神秘。然而，如果你仔细想想，这样也不比一个正常的神经网络难于理解。RNN可以看做是同一个神经网络的多次复制，每个神经网络模块会把消息传递给下一个。所以，如果我们将这个循环展开:

![2](/img/LSTM/2.png)

链式的特征揭示了RNN本质上是与序列和列表有关的。它们是用于此类数据的自然神经网络框架。

并且RNN也已经被人们应用了!在过去几年中，在语音识别，语言模型，翻译，图片描述等问题上应用RNN，已经取得了令人难以置信的成功，并在应用领域还在增长。我建议大家参考Andrej Karpathy 的博客文章------ [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/).来看看更丰富的RNN的成功应用。

而这些成功应用的关键就是使用了LSTM，这是一种特殊的RNN，在很多任务上都比标准的RNN表现更好。几乎所有的令人振奋的结果都是使用了LSTM 。这篇博文也会就LSTM进行展开。

### 长期依赖问题

RNN的关键点之一就是它们可以用来连接先前的信息到当前的任务上，例如使用过去的视频来推测对当前段的理解。如果RNN 可以做到这个，它们就变得非常有用，但是真的可以么？答案是，还有很多依赖因素。

有时候，我们仅仅需要知道先前的信息来执行当前的任务。例如，我们有一个语言模型基于当前词来预测下一个词。如果我们试着预测"the clouds are in the sky"最后的词，我们并不需要任何其他的上下文 -------因为下一个词很明显就应该是'sky'。在这样的场景中，相关的信息和预测的词位置之间的间隔是非常小的，RNN可以学会使用先前的信息。

![3](/img/LSTM3.png)

但是同样会有一些更复杂的场景。假设我们试着去预测'I grew up in France...I speak fluent French'最后的词。当前的信息建议下一个词可能是一种语言的名字，但是如果我们需要弄清楚是什么语言，我们是需要提前到离当前位置很远的'France'的上下文。这说明相关信息和当前预测位置之间的间隔肯定变得相当的大。

不幸的是，随着这个距离增长，RNN会丧失学习到连接如此远的信息的能力。

![4](/img/LSTM/4.png)

理论上，RNN绝对可以处理这样的长依赖问题。人们可以仔细挑选参数来解决这类初级形式的问题，但在实践中，RNN肯定不能够成功学习到这些知识。[Hochreiter (1991) [German]](http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf) and [Bengio, et al. (1994)](http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf)等人对该问题进行了深入的研究，他们发现了一些使训练RNN变得非常困难的根本原因.

幸运的是,LSTM并没有这个问题。

### LSTM网络

长短时记忆神经网络------一般就叫做LSTM-----是一种特殊的RNN类型。可以学习长期依赖信息。LSTM由 [Hochreiter & Schmidhuber (1997)](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)提出，并在近期被许多人改进和使用。在很多问题上，LSTM都取得了相当巨大的成功，并得到了广泛的使用。

LSTM就是明确设计来避免长期依赖的问题。记住长期的信息在实践中是LSTM的默认行为，而非需要付出很大的代价才能获得能力！

所有RNN都具有一种重复神经网络模块的链式的形式。在标准的RNN中，这个重复的模块只有一个非常的简单的结构，例如一个tanh层。

![5](/img/LSTM/5.png)

​						**标准RNN的重复模块包含单一的层**

LSTM同样是这样的结构，但是重复的模块拥有一个不同的结构。不同于单一神经网络层，这里是有四个，以一种非常特殊的方式进行交互。

![6](/img/LSTM/6.png)

​						**LSTM中的重复模块包含四个交互的层**

不要担心这里的细节。我们会一步一步地剖析LSTM解析图。我们先来熟悉一下图中使用的各种元素的图标。

![7](/img/LSTM/7.png)

在上面的图标中，每一条黑线传输着一整个向量，从一个节点的输出到其他节点的输入。粉色的圈代表pointwise的操作，诸如向量的和，而黄色的矩阵就是学习到的神经网络层。合在一起的线表示向量的连接，分开的线表示内容被复制，然后分发到不同的位置。

### LSTM的核心思想

LSTM的关键就是细胞状态，在图上方贯穿运行的水平线。

细胞状态类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在上面流畅保持不变会很容易。

![8](/img/LSTM/8.png)

LSTM有通过精心设计的称作为‘门’的结构来删除或者增加信息到细胞状态的能力。门是一种让信息选择式通过的方法。它们包含一个sigmoid神经网络层和pointwise乘法操作。

![9](/img/LSTM/9.png)

Sigmoid层输出0到1之间的数值，描述每个部分有多少量可以通过。0代表‘不允许任何量通过’，1指‘允许任意量通过’!

LSTM拥有三个门，来保护和控制单元状态。

### 逐步理解LSTM

在LSTM中的第一步是决定我们会从单元状态中丢去什么信息。这个决定通过一个称为‘忘记门’层完成。该门会读取$h_{t-1}$和$x_t$，在每个单元状态$c_{t-1}$输出一个在0和1之间的数值。1表示‘完全保留’，0表示‘完全舍弃’。

让我们回到语言模型的例子中来基于已经看到的预测下一个词。在这个问题中，单元状态可能包含当前主语的性别，因此正确的代词可以被选择出来。当我们看到新的主语，我们希望忘记旧的主语。

![10](/img/LSTM/10.png)

下一步是确定什么样的新信息被存放在单元状态中。这里包含两个部分。第一，sigmoid层称‘输入门层’决定什么值我们将要更新。然后，一个tanh层创建一个新的候选值向量，$\tilde{C}_t$会被加入到状态中。下一步，我们会讲这两个信息来产生对状态的更新。

在我们语言模型的例子中，我们希望增加新主语的性别到单元状态中，来代替旧的要忘记的主语。

![11](/img/LSTM/11.png)

​						**确定更新的信息**

现在是更新旧单元状态的时间了，$C_{t-1}$更新为$C_{t}$。前面的步骤已经决定了将会做什么，我们现在就是实际去完成。

我们把旧状态与$f_t$相乘，丢弃掉我们确定需要丢弃的信息。接着加上$i_t* \tilde C_{t}$。这就是新的候选值，根据我们决定更新每个状态的程度进行变化。在语言模型的例子中，这就是我们实际根据前面确定的目标，丢弃旧代词的性别信息并添加新信息的地方。

![12](/img/LSTM/12.png)

最终，我们需要确定输出什么值。这个输出将会基于我们的单元状态，但是也是一个过滤后的版本。首先，我们运行一个sigmoid层来确定单元状态的哪个部分将输出出去。接着，我们把单元的状态通过tanh进行处理(得到一个在-1到1之间的值)并将它和sigmoid门的输出相乘，最终我们仅仅会输出我们确定输出的那部分。

在语言模型的例子中，因为它就看到了一个**代词**，可能需要输出与一个**动词**相关的信息。例如，可能输出是否代词是单数还是负数，这样如果是动词的话，我们也知道动词需要进行的词形变化。

![13](/img/LSTM/13.png)

### LSTM的变体

到目前为止我们还在介绍正常的LSTM。但是不是所有的LSTM都长成一个样子的。实际上，几乎所有包含LSTM的论文都采用了微小的变体。差异非常小，但是也值得提出来。

其中一个流行的LSTM的变体，就是[Gers & Schmidhuber (2000)](ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf)提出的，增加了'people connection'。就是说，我们让门层也会接受单元状态的输入。

![14](/img/LSTM/14.png)

上面的图例中，我们增加了peephole(窥视)到每个门上，但是许多论文会加入一些窥视，而非所有都加。

另一种变化时使用耦合的忘记和输入门。它不是分开决定要忘记和添加的信息，而是一起做出这些决定。我们会在某个位置输入一些东西的时候才会忘记它，输入新值的时候会忘记旧信息的状态。

![15](/img/LSTM/15.png)

另一个改动较大的变种是Gated Recurrent Unit (GRU)，是由[Cho, et al. (2014)](http://arxiv.org/pdf/1406.1078v3.pdf)提出的。它将忘记门和输入门合成了一个单一的'更新门'.同样还混合了单元状态和隐藏状态以及其他一些改动。最终的模型比标准的LSTM模型要简单，也是非常流行的变体。

![16](/img/LSTM/16.png)

这里只是部分流行的LSTM变体。当然还有很多其他的，如 [Yao, et al. (2015)](http://arxiv.org/pdf/1508.03790v2.pdf)提出的Depth Gated RNN 。还有用一些完全不同的观点来解决长期依赖的问题，如 [Koutnik, et al. (2014)](http://arxiv.org/pdf/1402.3511v1.pdf)提出的Clockword RNN 。

哪个变种最好？变种的不同之处是否有用？[Greff, et al. (2015)](http://arxiv.org/pdf/1503.04069.pdf)给出了流行变种的比较，结论是它们基本上是一样的。[Jozefowicz, et al. (2015)](http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)则在1万种RNN架构上进行了测试，发现在某些任务上，一些架构比LSTM取得了更好的结果。

### 结论

刚开始，我提到人们通过使用RNN获得了显著的结果。基本上所有这些都是使用LSTM实现的，而且它们的确在大多数任务上做的更好！

LSTM写成一组公式，看起来很吓人。希望 在这篇文章中一步一步去理解它，可以减少你的困惑。

LSTM是我们在RNN中获得的重要成功。自然地，我们也会考虑：哪里会有更加重大的突破呢？研究人员间普遍的观点是:“是的！下一步已经有了----那就是**注意力**”这个想法是让RNN的每个步骤从一些较大的信息集合中选择信息。例如，如果您使用RNN创建描述图像的标题，则可能会选择图像的一部分，根据这部分的信息来产生输出的词。实际上，[Xu, *et al.*(2015)](http://arxiv.org/pdf/1502.03044v2.pdf)已经这么做了----如果你希望深入探索**注意力**，可能这就是一个有趣的开始！还有一些使用注意力的研究成果相当振奋人心，看起来有更多的大小亟待探索.....

注意力也不是RNN研究领域中唯一的发展方向。例如，[Kalchbrenner, *et al.* (2015)](http://arxiv.org/pdf/1507.01526v1.pdf) 提出的Grid LSTM看起来也很有前途。使用生成模型的RNN，诸如[Gregor, *et al.* (2015)](http://arxiv.org/pdf/1502.04623.pdf), [Chung, *et al.* (2015)](http://arxiv.org/pdf/1506.02216v3.pdf), 和 [Bayer & Osendorfer (2015)](http://arxiv.org/pdf/1411.7610v3.pdf)提出的模型同样有趣。在过去几年中，RNN的研究已经相当的激动人心了，未来这些神经网络会带来更多的惊喜！

### 致谢

我很感谢一些人帮助我更好地了解了LSTM，并提供关于这篇文章的反馈和评论。我非常感谢Google同事们的有用反馈，特别是Oriol Vinyals,Greg Corrado,Jon Shlens，Luke Vilnis和Ilya Sutskever。我也感谢许多其他朋友和同事花时间帮助我，包括达里奥·阿莫德迪和雅各布·斯坦哈特。 我非常感谢Kyunghyun Cho对我的图表进行了深思熟虑。



>文章译自 [Understanding LSTM Networks]<http://colah.github.io/posts/2015-08-Understanding-LSTMs/> 作者colah

