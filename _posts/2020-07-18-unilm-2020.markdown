---
layout:     post
title:      "统一预训练语言模型"
subtitle:   "Unified Language Model Pre-training for Natural Language Understanding and Generation"
date:       2020-07-18 10:00:00
author:     "Wwt"
header-img: "img/unilm/bg.png"
catalog: true
tags:   
    - 文本摘要
    - 预训练模型
    - NLP
---

**UniLM**论文全名为**Unified Language Model Pre-training for Natural Language Understanding and Generation**，译为**自然语言理解与生成的统一预训练语言模型**，来自于微软研究院。

>本文参考自[UniLM论文阅读笔记](https://zhuanlan.zhihu.com/p/113380840)，部分有删改

### 介绍

目前，预训练模型(Language model) 大幅地提高了各种自然语言处理任务的水平。它一般使用大量文本数据，通过上下文来预测单词，从而学习到文本上下文的文本表示，并且可以通过微调来适应后续任务。不同类型的预训练语言模型一般采用不同的预测任务和训练目标。ELMO模型学习两个单向语言模型(unidirectional LM):前向语言模型从左到右读文本进行编码，后向语言模型从右到左读取文本进行编码。GPT模型使用Transformer编码解码，从左到右的逐字预测文本序列。BERT模型使用一个双向Transformer编码器通过掩字上下文来预测该掩蔽字。

尽管BERT模型已经显著地提高了大量自然语言理解任务的效果，但是由于它的**双向性**使得它很难应用于自然语言处理任务。



因此，作者提出一个新的统一预训练语言模型(uniLM),既可以应用于自然语言理解(NLU)任务，又可以应用于自然语言生成(NLG)任务。uniLM模型的框架与BERT一致，是一个多层Transformer网络构成，但训练方式不同，它是通过联合训练三种不同的目标函数的无监督语言得到。

为了使三种不同的目标函数运用到同一种模型框架中，作者设计了三类完形填空任务，像BERT模型一样，去预测被掩的token。下面详细介绍如何设计这三类完形填空任务。

![1](/img/unilm/1.png)

### 统一预训练语言模型

模型框架如图所示，在预训练阶段，unilm模型通过三种不同目标函数的语言模型(包括：双向语言模型，单向语言模型和序列到序列语言模型)，去共同优化同一个Transformer网络：**为了控制对将要预测的token可见到的上下文，作者使用了不同的self-attention mask 来实现。**换句话说，就是通过不同的掩码来控制预测单词的可见上下文词语数量，实现不同的模型表征。



**单向语言模型**：分别从左到右和从右到左两种两种，从左到右，即仅通过被掩蔽token的左侧所有本文来预测被掩蔽的token；从右到左，则是仅通过被掩蔽token的右侧所有本文来预测被掩蔽的token。

**双向语言模型**：与BERT模型一致，在预测被掩蔽token时，可以观察到所有的token。

**序列到序列语言模型**：如果被掩蔽token在第一个文本序列中，那么仅可以使用第一个文本序列中所有token，不能使用第二个文本序列的任何信息；如果被掩蔽token在第二个文本序列中，那么使用一个文本序列中所有token和第二个文本序列中被掩蔽token的左侧所有token预测被掩蔽token.

### 输入表示

对于单向语言模型，模型输入是一个单独的文本，对于双向语言模型和序列到序列的语言模型，模型输入是一个文本pair对。并且在第一个文本开始前加上[SOS] token，在每一个文本末尾加上[EOS]token，用于表示不同文本的分割，(与BERT相似，只不过将[CLS]换成[SOS]，将[SEP]换成了[EOS])。在NLG任务上，[EOS]还可以作为文本生成结束的标志。输入表征跟BERT模型也是一致的，由字向量，位置向量和句子段落向量组合而成。在文本处理过程中，使用wordpiece把token都处理成了subword(猜测时增强模型的生成能力)。

因为，unilm模型使用多语言模型任务进行训练，所以，作者对于不同的语言模型使用了不同的句子段落向量。

### 多层Transformer

模型结构与Bert一致，每个transformer中使用多头self-attention结构组成，公式如下：
$$
A_l =softmax(\frac{QK^T}{\sqrt{}d_k}+M)V_l
$$
其中， $ M \in R^{\mid x\mid*\mid x \mid}$表示该字词是否可以被其他字词看到，如上图所示，不同的任务，我们赋予不同的掩码。例如：双向语言模型，我们就允许所有字词都可以相互看见。

### 预训练目标

作者设计了三种类型的完形填空任务，共同优化同一个transformer框架。在完形填空任务中，我们在文本中随机选择了一些token，使用特殊标记[MASK]进行替换；将文本输入到Transformer网络，计算出对对应的输出向量，在通过softmax分类器预测[MASK]到底属于字典中哪一个token。

unilm模型参数通过最小化预测token和标准token的交叉熵来优化。三种类型的完形填空任务可以完成不同的语言模型运用相同的程序训练。

**单向语言模型**：有从右到左和从左到右两者，以从左到右为例介绍，每个特殊[MASK]的预测，仅采用它自身和其左侧的token进行编码。例如，我们预测序列$x_1x_2[mask]x_4$中的[mask]，我们仅可以利用x_1、x_2和它自身进行编码。具体操作，如上图所示，使用一个上三角矩阵来作为掩码矩阵。阴影部分为$-\infty$，空白部分为0。

**双向语言模型**：跟BERT模型一致，每个特殊标记[Mask]的预测，可以使用所有的token进行编码。例如，我们预测序列$x_1x_2[mask]x_4$中的[mask]，我们仅可以利用$x_1$、$x_2$、$x_4$和它自身进行编码。具体操作，如上图所示，使用全0矩阵来作为掩码矩阵。

**序列到序列语言模型：**如果预测的特殊标记[Mask]出现在第一段文本中时，仅可以使用第一段文本中所有的token进行预测；如果预测的特殊标记[Mask]出现在第二段文本中时，可以采用第一段文本中所有的token，和第二段文本中该预测标记的左侧所有token以及它本身。例如，我们预测序列 中的[mask1]时，除去[SOS]和[EOS]，我们仅可以利用$x_1$、$x_2$、$x_4$和它自身进行编码；预测[mask2]时除去[SOS]和[EOS]，我们仅可以利用$x_1$、$x_2$、[mask1]、$x_4$、$x_5$、$x_6$和它自身进行编码。具体操作，如上图所示。

由于在训练时，将源文本和目标文本结合进入模型，使模型可以含蓄地学习到两个文本之间关系，可以做到seq-to-seq的效果。

**NSP任务：**对于双向语言模型，与bert模型一样，也进行下一个句子预测。如果是第一段文本的下一段文本，则预测1；否则预测0。

**以上就是模型核心部分，用一句来总结一下，就是对于不同的语言模型，我们可以仅改变self-attention mask，就可以完成联合训练。**

### 总结

以上就是unilm模型的全部介绍，重点还是如何设计统一的语言模型，通过修改mask的方式，实现统一程序做不同的任务，也可以理解为，通过不同任务，去优化同一份模型参数，在不同任务中，模型参数是共享的。

>另外，在序列到序列语言模型，在预训练过程中， maks token的选取，是可以出现在第一个文本序列中，也可以出现在第二个文本序列中，但是在微调阶段时， mask token仅出现在第二个文本序列中，因为我们需要通过第一个文本生成第二个文本。

