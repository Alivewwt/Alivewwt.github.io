---
layout:     post
title:      "Keras入门-中篇"
subtitle:   "Keras"
date:       2017-07-19 11:00:00
author:     "Wwt"
header-img: "img/keras_1/bg.jpg"
catalog: true
tags:   
    - Keras
    - 基础教程
---
### Keras的网络层

#### 关于Keras的层"Layer"

​	所有的Keras层对象都有如下方法：

- **layer.get_weights()**:返回层的权重(numpy array)
- **layer.set_weights(weights)**:从numpy array中将权重加载到该层中，要求numpy array的形状与*layer.get_weights()的形状相同
- **layer.get_config()**:返回当前层配置信息的字典，层也可以借由配置信息重构

```python
layer = Dense(32)
config = layer.get_config()
reconstructed_layer = Dense.from_config(config)
```

如果层仅有一个计算节点(即该层不是共享层)，则可以通过下列方法获得输入张量、输出张量、输入数据的性状和输出数据的形状。

- **layer.input**
- **layer.output**
- **layer.input_shape**
- **layer.output_shape**

如果该层有多个计算节点，可以使用下列方法

- **layer.get_inout_at(node_index)**
- **layer.get_output_at(node_index)**
- **layer.get_input_shape_at(node_index)**
- **layer.get_output_shape_at(node_index)**

#### 常用层

常用层对应于core模块，core内部定义了一系列常用的网络层，包括全连接层、激活层等

###### Dense层

```python
keras.layers.core.Dense(units, activation=None)
```

Dense就是常用的全连接层，所实现的运算是**output = activation(dot(input, kernel)+bias)**。其中**activation**是逐元素计算的激活函数，**kernel**是本层的权值矩阵，**bias**为偏置向量，只有当**use_bias=True**才会添加。

如果本层的输入数据的维度大于2，则会先被压为与**kernel**相匹配的大小。

这里是一个使用示例：

```python
# 序贯模型中第一层
model = Sequential()
model.add(Dense(32, input_shape=(16,)))
# 模型接受输入数组(*,16)
# 输出数组 (*, 32)

#第一层之后无需再具体指定输入的长度
model.add(Dense(32))
```

**参数**

- units:大于0的整数，代表该层的输出维度。
- activation：激活函数，为预定义的激活函数，或逐预算(element-wise)的Theano函数。如果不指定该参数，将不会用任何激活函数(即使用线性激活函数：a(x)=x)

**输入**

形如(nb_samples,...,input_shape[1])的nD张量，最常见的情况为(nb_samples,input_dim)的2D张量

**输出**

形如(nb_samples,...units)的nD张量，最常见的情况为(nb_samples,output_dim)的2D张量

###### **Activation层**

```python
keras.layers.core.Activation(activation)
```

激活层对一个层的输出施加激活函数

**参数**

- activation:将要使用的激活函数，为预定义激活函数名或一个Tensorflow/Theano的函数。

**输入shape**

任意，当使用激活层作为第一层时，要指定**input_shape**

**输出shape**

与输入shape相同

###### **Dropout层**

```python
keras.layers.core.Dropout(rate, noise_shape=None, seed=None)
```

为输入数据施加Dropout。Dropout将在训练过程中每次更新参数随机断开一定百分比(rate)的输入神经元，Dropout层用于防止过拟合。

**参数**

- rate:0~1的浮点数，控制需要断开的神经元比例
- noise_shape:整数张量，为将要应用在输入上的二值Dropout mask 的shape，例如你的输入为(batch_size,timesteps,features)，并且你希望在各个时间步上的Dropout mask都相同，则可传入noise_shape=(batch_size,1,features)。
- seed:整数，使用的随机数种子

###### Flatten层

```python
keras.layers.core.Flatten()
```

Flatten层用来将输入“压平”，即把多维的输入一维化，常用在从卷积层到全连接层的过渡。Flatten不影响batch的大小。

**例子**

```python
model = Sequential()
model.add(Convolution2D(64, 3, 3,
            border_mode='same',
            input_shape=(3, 32, 32)))
# 模型输出(None, 64, 32, 32)

model.add(Flatten())
# 模型输出 == (None, 65536)
```

###### **Reshape层**

```python
keras.layers.core.Reshape(target_shape)
```

Reshape层用来将输入shape转换为特定的shape

**参数**

- target_shape:目标shape，为整数的tuple，不包含样本数目的维度(batch大小)

**输入shape**

任意，但输入的shape必须固定。当使用该层为模型首层时，需要指定**input_shape**参数

**输出shape**

(batch_size,)+target_shape

**例子**

```python
# 序贯模型的第一层
model = Sequential()
model.add(Reshape((3, 4), input_shape=(12,)))
# model.output_shape == (None, 3, 4)
# “None”是样本数目维

# 作为Sequential 模型的中间层
model.add(Reshape((6, 2)))
# model.output_shape == (None, 6, 2)

# 支持使用"-1"作为维度
model.add(Reshape((-1, 2, 2)))
#  model.output_shape == (None, 3, 2, 2)
```

##### 卷积层

###### **Conv1D层**

```python
keras.layers.convolutional.Conv1D(filters, kernel_size, strides=1, padding='valid', dilation_rate=1, activation=None)
```

一维卷积层(即时域卷积)，用以在一维输入信号上进行邻域滤波。当使用该层作为首层时，需要提供关键字参数**input_shape**。例如(10,128)代表一个长为10 的序列，序列中每个信号为128向量。而(None ,128)代表变长的128维向量序列。

该层生成将输入信号与卷积核按照单一的空域(或时域)方向进行卷积。如果**use_bias=True**，则还会加上一个偏置项，若**activation**不为Noe,则输出为经过激活函数的输出。

**参数**

- filters:卷积核的数目(即输出的维度)
- kernel_size:整数或由单个整数构成的list/tuple，卷积核的空域或时域窗长度
- strides:整数或由单个整数构成的list/tuple，为卷积的步长。任何不为1的strides均为任何不为1的dilation_rata均不兼容
- padding:补0策略，为"valid"，"same"或"casual"，"casual"将产生因果(膨胀的)卷积，即output[t]不依赖于input[t+1:]。当对不能违反事件顺序的时序信号建模时有用。“valid”代表只进行有效的卷积，即对边界数据不处理。“same”代表保留边界处的卷积结果，通常会导致输出shape与输入shape相同。
- activation:激活函数，为预定义的激活函数名,或逐元素的Theano函数。如果不指定该函数，将不会使用任何激活函数(即使用线性激活函数:a(x)=x)

**输入shape**

形如(samples,steps,input_dim)的3D张量

**输出shape**

形如(samples,new_steps,nb_filter)的3D张量，因为有向量填充的原因，steps的值会改变

###### **Conv2D层**

```python
keras.layers.convolutional.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None)
```

二维卷积层，即对图像的空域卷积。该层对二维进行滑动窗卷积，当使用该层作为第一层时，应提供**input_shape**参数。例如**input_shape=(128,128,3)**代表128*128的彩色RGB图像(**data_format='channel_last'**)

**参数**

- filters:卷积核的数目(即输出的维度)
- kernel_size:单个整数或由两个整数构成的list/tuple,卷积核的宽度和长度。如为单个整数，则表示在各个空间维度的相同长度
- strides:单个整数或由两个整数构成的list/tuple，为卷积的步长。如为单个整数，则表示在各个空间维度的相同步长。任何不为1的strides均为任何不为1的dilation_rata均不兼容
- padding:补0策略，为“valid”, “same” 。“valid”代表只进行有效的卷积，即对边界数据不处理。“same”代表保留边界处的卷积结果，通常会导致输出shape与输入shape相同。
- activation：激活函数，为预定义的激活函数名（参考[激活函数](http://keras-cn.readthedocs.io/en/latest/other/activations)），或逐元素（element-wise）的Theano函数。如果不指定该参数，将不会使用任何激活函数（即使用线性激活函数：a(x)=x）
- dilation_rate：单个整数或由两个个整数构成的list/tuple，指定dilated convolution中的膨胀比例。任何不为1的dilation_rata均与任何不为1的strides均不兼容。
- data_format:字符串，"channels_first"或"channel_last"之一，代表图像的通道维的位置。该参数是Keras1.x中的image_dim_ordering,“channel_last”对应原本的"tf"，“channel_first”对应原本的"th".以128*128的RGB图像为例，“channel_first”应将数据组织为(3,128,128),而"channels_last"应将数据组织为(128,128,3).该参数的默认值是`~/.keras/keras.json`中设置的值，若从未设置过，则为“channels_last”。

**输出shape**

'channels_first'模式下，为形如(samples,nb_filter,new_rows,new_cols)的4D张量

'channels_last'模式下，为形如(samples,new_rows,new_cols,nb_filter)的4D张量

输出行列数可能会因为填充方法而改变

##### 池化层

###### **MaxPooling1D层** 

```python
keras.layers.pooling.MaxPooling1D(pool_size=2, strides=None, padding='valid')
```

对时域1D信号进行最大值池化

**参数**

- pool_size:整数，池化窗口大小
- strides:整数或None，下采样因子，例如设2将会使得输出shape为输入的一半，若为None则默认值为pool_size
- padding:"valid"或者"same"

**输入shape**

- 形如(samples,steps,features)的3D张量

**输出shape**

- 形如(samples,downsampled_steps,features)的3D张量

###### **MaxPooling2D层**

```python
keras.layers.pooling.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)
```

**参数**

- pool_size:整数或长为2的整数tuple，代表在两个方向(竖直，水平)上的下采样因子，如取(2,2)将使图片在两个维度上均变为原长的一半。为整数意为各个维度值相同为该数字。
- strindes:整数或长为2的整数tuple，或者None，步长值。
- border_mode：‘valid’或者‘same’
- data_format：字符串，“channels_first”或“channels_last”之一，代表图像的通道维的位置。该参数是Keras 1.x中的image_dim_ordering，“channels_last”对应原本的“tf”，“channels_first”对应原本的“th”。以128x128的RGB图像为例，“channels_first”应将数据组织为（3,128,128），而“channels_last”应将数据组织为（128,128,3）。该参数的默认值是`~/.keras/keras.json`中设置的值，若从未设置过，则为“channels_last”。

**输入shape**

'channels_first’模式下，为形如（samples，channels, rows，cols）的4D张量

‘channels_last’模式下，为形如（samples，rows, cols，channels）的4D张量

**输出shape**

‘channels_first’模式下，为形如（samples，channels, pooled_rows, pooled_cols）的4D张量

‘channels_last’模式下，为形如（samples，pooled_rows, pooled_cols，channels）的4D张量

###### **AveragePooling1D层**

```python
keras.layers.pooling.AveragePooling1D(pool_size=2, strides=None, padding='valid')
```

对时域1D信号进行平均值池化

**参数**

- pool_size:整数，池化窗口大小
- strides:整数或None,下采样因子，例如设2将会使得输出shape为输入的一半，若为None则默认值为pool_size。
- padding：‘valid’或者‘same’

**输入shape**

- 形如（samples，steps，features）的3D张量

**输出shape**

- 形如（samples，downsampled_steps，features）的3D张量

###### AveragePooling2D层

```python
keras.layers.pooling.AveragePooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)
```

为空域信号施加平均值池化

**参数**

- pool_size：整数或长为2的整数tuple，代表在两个方向（竖直，水平）上的下采样因子，如取（2，2）将使图片在两个维度上均变为原长的一半。为整数意为各个维度值相同且为该数字。
- strides：整数或长为2的整数tuple，或者None，步长值。
- border_mode：‘valid’或者‘same’
- data_format：字符串，“channels_first”或“channels_last”之一，代表图像的通道维的位置。该参数是Keras 1.x中的image_dim_ordering，“channels_last”对应原本的“tf”，“channels_first”对应原本的“th”。以128x128的RGB图像为例，“channels_first”应将数据组织为（3,128,128），而“channels_last”应将数据组织为（128,128,3）。该参数的默认值是`~/.keras/keras.json`中设置的值，若从未设置过，则为“channels_last”。

**输入shape**

‘channels_first’模式下，为形如（samples，channels, rows，cols）的4D张量

‘channels_last’模式下，为形如（samples，rows, cols，channels）的4D张量

**输出shape**

‘channels_first’模式下，为形如（samples，channels, pooled_rows, pooled_cols）的4D张量

‘channels_last’模式下，为形如（samples，pooled_rows, pooled_cols，channels）的4D张量

###### GlobalMaxPooling1D层

```python
keras.layers.pooling.GlobalMaxPooling1D()
```

对时间信号的全局最大池化

**输入shape**

- 形如(samples,steps,features)的3D张量

**输出shape**

- 形如(samples,features)的2D张量

###### GlobalAveragePooling1D层

```python
keras.layers.pooling.GlobalAveragePooling1D()
```

为时域信号施加全局平均值池化

**输入shape**

- 形如(samples,steps,features)的3D张量

**输出shape**

- 形如(samples,features)的2D张量

###### GlobalMaxPooling2D层

```python
keras.layers.pooling.GlobalMaxPooling2D(dim_ordering='default')
```

为空域信号施加全局最大值池化

**参数**

- data_format：字符串，“channels_first”或“channels_last”之一，代表图像的通道维的位置。该参数是Keras 1.x中的image_dim_ordering，“channels_last”对应原本的“tf”，“channels_first”对应原本的“th”。以128x128的RGB图像为例，“channels_first”应将数据组织为（3,128,128），而“channels_last”应将数据组织为（128,128,3）。该参数的默认值是`~/.keras/keras.json`中设置的值，若从未设置过，则为“channels_last”。

**输入shape**

- ‘channels_first’模式下，为形如（samples，channels, rows，cols）的4D张量
- ‘channels_last’模式下，为形如（samples，rows, cols，channels）的4D张量

**输出shape**

-  形如(nb_samples, channels)的2D张量

###### GlobalAveragePooling2D层

```python
keras.layers.pooling.GlobalAveragePooling2D(dim_ordering='default')
```

为空域信号施加全局平均值池化

**参数**

- data_format：字符串，“channels_first”或“channels_last”之一，代表图像的通道维的位置。该参数是Keras 1.x中的image_dim_ordering，“channels_last”对应原本的“tf”，“channels_first”对应原本的“th”。以128x128的RGB图像为例，“channels_first”应将数据组织为（3,128,128），而“channels_last”应将数据组织为（128,128,3）。该参数的默认值是`~/.keras/keras.json`中设置的值，若从未设置过，则为“channels_last”。

**输入shape**

‘channels_first’模式下，为形如（samples，channels, rows，cols）的4D张量

‘channels_last’模式下，为形如（samples，rows, cols，channels）的4D张量

**输出shape**

形如(nb_samples, channels)的2D张量

##### 局部连接层LocallyConnceted

###### LocallyConnected1D层

```python
keras.layers.local.LocallyConnected1D(filters, kernel_size, strides=1, padding='valid', data_format=None, activation=None, 
```

LocallyConnected1D层与Conv1D工作方式类似，唯一的区别是不进行权值共享。即施加在不同输入位置的滤波器是不一样的。

**参数**

- filters：卷积核的数目（即输出的维度）
- kernel_size：整数或由单个整数构成的list/tuple，卷积核的空域或时域窗长度
- strides：整数或由单个整数构成的list/tuple，为卷积的步长。任何不为1的strides均与任何不为1的dilation_rata均不兼容
- padding：补0策略，目前仅支持`valid`（大小写敏感），`same`可能会在将来支持。
- activation：激活函数，为预定义的激活函数名（参考[激活函数](https://keras-cn.readthedocs.io/en/latest/other/activations)），或逐元素（element-wise）的Theano函数。如果不指定该参数，将不会使用任何激活函数（即使用线性激活函数：a(x)=x）

**输入shape**

形如(samples，steps，input_dim)的3D张量

**输出shape**

形如（samples，new_steps，nb_filter）的3D张量，因为有向量填充的原因，`steps`的值会改变

###### LocallyConnected2D层

```python
keras.layers.local.LocallyConnected2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, activation=None)
```

**LocallyConnected2D**层与**Convolution2D**工作方式类似，唯一的区别是不进行权值共享。即施加在不同输入patch的滤波器是不一样的，当使用该层作为模型首层时，需要提供参数**input_dim**或**input_dim**参数。参数含义参考**Convolution2D**

**参数**

- filters：卷积核的数目（即输出的维度）
- kernel_size：单个整数或由两个整数构成的list/tuple，卷积核的宽度和长度。如为单个整数，则表示在各个空间维度的相同长度。
- strides：单个整数或由两个整数构成的list/tuple，为卷积的步长。如为单个整数，则表示在各个空间维度的相同步长。
- padding：补0策略，目前仅支持`valid`（大小写敏感），`same`可能会在将来支持。
- activation：激活函数，为预定义的激活函数名（参考[激活函数](https://keras-cn.readthedocs.io/en/latest/other/activations)），或逐元素（element-wise）的Theano函数。如果不指定该参数，将不会使用任何激活函数（即使用线性激活函数：a(x)=x）

**输入shape**

‘channels_first’模式下，输入形如（samples,channels，rows，cols）的4D张量

‘channels_last’模式下，输入形如（samples，rows，cols，channels）的4D张量

**输出shape**

‘channels_first’模式下，为形如（samples，nb_filter, new_rows, new_cols）的4D张量

‘channels_last’模式下，为形如（samples，new_rows, new_cols，nb_filter）的4D张量

输出的行列数可能会因为填充方法而改变

###### 例子

```python
# 在3个32*32的图片上使用64个3*3的过滤器
# with `data_format="channels_last"`:
model = Sequential()
model.add(LocallyConnected2D(64, (3, 3), input_shape=(32, 32, 3)))
# now model.output_shape == (None, 30, 30, 64)
# notice that this layer will consume (30*30)*(3*3*3*64) + (30*30)*64 parameters

# 在顶部添加一个不共享的3*3卷积, 有32个过滤器输出:
model.add(LocallyConnected2D(32, (3, 3)))
# now model.output_shape == (None, 28, 28, 32)
```

##### 循环层Recurrent

###### Recurrent层

```python
keras.layers.recurrent.Recurrent(return_sequences=False, go_backwards=False, stateful=False, unroll=False, implementation=0)
```

这是循环层的抽象类，请不要在模型中直接应用该层(因为它是抽象类，无法实例化任何对象)。请使用它的子类**LSTM**，**GRU**或**SimpleRNN**

所有的循环层(LSTM,GRU,SimpleRNN)都服从本层的性质，并接受本层指定的所有关键字参数。

**参数**

- weights：numpy array的list，用以初始化权重。该list形如`[(input_dim, output_dim),(output_dim, output_dim),(output_dim,)]`
- return_sequences：布尔值，默认`False`，控制返回类型。若为`True`则返回整个序列，否则仅返回输出序列的最后一个输出
- go_backwards：布尔值，默认为`False`，若为`True`，则逆向处理输入序列并返回逆序后的序列
- stateful：布尔值，默认为`False`，若为`True`，则一个batch中下标为i的样本的最终状态将会用作下一个batch同样下标的样本的初始状态。
- unroll：布尔值，默认为`False`，若为`True`，则循环层将被展开，否则就使用符号化的循环。当使用TensorFlow为后端时，循环网络本来就是展开的，因此该层不做任何事情。层展开会占用更多的内存，但会加速RNN的运算。层展开只适用于短序列。
- implementation：0，1或2， 若为0，则RNN将以更少但是更大的矩阵乘法实现，因此在CPU上运行更快，但消耗更多的内存。如果设为1，则RNN将以更多但更小的矩阵乘法实现，因此在CPU上运行更慢，在GPU上运行更快，并且消耗更少的内存。如果设为2（仅LSTM和GRU可以设为2），则RNN将把输入门、遗忘门和输出门合并为单个矩阵，以获得更加在GPU上更加高效的实现。注意，RNN dropout必须在所有门上共享，并导致正则效果性能微弱降低。
- input_dim：输入维度，当使用该层为模型首层时，应指定该值（或等价的指定input_shape)
- input_length：当输入序列的长度固定时，该参数为输入序列的长度。当需要在该层后连接`Flatten`层，然后又要连接`Dense`层时，需要指定该参数，否则全连接的输出无法计算出来。注意，如果循环层不是网络的第一层，你需要在网络的第一层中指定序列的长度（通过`input_shape`指定）。

**输入shape**

形如（samples，timesteps，input_dim）的3D张量

**输出shape**

如果**return_sequences=True**：返回形如（samples，timesteps，output_dim）的3D张量，否则，返回形如(samples,output_dim)的2D张量

**例子**

```
#第一层是序贯模型
model = Sequential()
model.add(LSTM(32, input_shape=(10, 64)))
# now model.output_shape == (None, 32)
# note: `None` is the batch dimension.

model = Sequential()
model.add(LSTM(32, input_dim=64, input_length=10))

# 对于后面的层，不需指定输入长度

#堆叠循环层，必须使用 return_sequences=True
# 在任意一个循环层中进入另一个循环层.
# 你只需指定第一层的输入大小.
model = Sequential()
model.add(LSTM(64, input_dim=64, input_length=10, return_sequences=True))
model.add(LSTM(32, return_sequences=True))
model.add(LSTM(10))
```

**指定RNN初始状态的注意事项**

可以通过设置**initial_state**用符号式的方式指定RNN层的初始状态。即，**initial_stat**的值应该为一个tensor或一个tensor列表，代表RNN层的初始状态。

也可以通过设置**reset_states**参数用数值的方法设置RNN的初始状态，状态的值应该为numpy数组或numpy数组的列表，代表RNN层的初始状态。

**屏蔽输入数据(Masking)**

循环层支持通过时间步变量对输入数据进行Masking，如果想将输入数据的一部分屏蔽掉，请使用Embedding层并将参数**mask_zero**设为**True**。

**使用状态RNN的注意事项**

可以将RNN设置为‘stateful’，意味着由每个batch计算出的状态都会被重用于初始化下一个batch的初始状态。状态RNN假设连续的两个batch之中，相同下标的元素有一一映射关系。

要启用状态RNN，请在实例化层对象时指定参数**stateful=Tru**，并在Sequential模型使用固定大小的batch：通过在模型的第一层传入**batch_size=(...)**和**input_shape**来实现。在函数式模型中，对所有的输入都要指定相同的**batch_size**。

如果要将循环层的状态重置，请调用.**reset_states()**，对模型调用将重置模型中所有状态RNN的状态。对单个层调用则只重置该层的状态。

###### SimpleRNN层

```python
keras.layers.recurrent.SimpleRNN(units, activation='tanh', use_bias=True,)
```

全连接RNN网络，RNN的输出会被回馈到输入。

**参数**

- units：输出维度
- activation：激活函数，为预定义的激活函数名（参考[激活函数](https://keras-cn.readthedocs.io/en/latest/other/activations)）
- use_bias: 布尔值，是否使用偏置项

###### GRU层

```python
keras.layers.recurrent.GRU(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True,
```

门控循环单元

**参数**

- units：输出维度
- activation：激活函数，为预定义的激活函数名（参考[激活函数](https://keras-cn.readthedocs.io/en/latest/other/activations)）
- use_bias: 布尔值，是否使用偏置项

###### LSTM层

```python
keras.layers.recurrent.LSTM(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True,
```

Keras长短期记忆模型。

**参数**

- units：输出维度
- activation：激活函数，为预定义的激活函数
- recurrent_activation: 为循环步施加的激活函数
- use_bias: 布尔值，是否使用偏置项

##### 嵌入层Embedding

###### Embedding层

```python
keras.layers.embeddings.Embedding(input_dim, output_dim, embeddings_initializer='uniform', embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None)
```

嵌入层将正整数(小标)转换为具有固定大小的向量,如[[4],[20]]->[[0.25,0.1],[0.6,-0.2]]

Embedding层只能作为模型的第一层

**参数**

- input_dim：大或等于0的整数，字典长度，即输入数据最大下标+1
- output_dim：大于0的整数，代表全连接嵌入的维度
- embeddings_initializer: 嵌入矩阵的初始化方法，为预定义初始化方法名的字符串，或用于初始化权重的初始化器。参考[initializers](https://keras-cn.readthedocs.io/en/latest/other/initializations)
- embeddings_regularizer: 嵌入矩阵的正则项，为[Regularizer](https://keras-cn.readthedocs.io/en/latest/other/regularizers)对象
- embeddings_constraint: 嵌入矩阵的约束项，为[Constraints](https://keras-cn.readthedocs.io/en/latest/other/constraints)对象
- mask_zero：布尔值，确定是否将输入中的‘0’看作是应该被忽略的‘填充’（padding）值，该参数在使用[递归层](https://keras-cn.readthedocs.io/en/latest/layers/recurrent_layer)处理变长输入时有用。设置为`True`的话，模型中后续的层必须都支持masking，否则会抛出异常。如果该值为True，则下标0在字典中不可用，input_dim应设置为|vocabulary| + 2。
- input_length：当输入序列的长度固定时，该值为其长度。如果要在该层后接`Flatten`层，然后接`Dense`层，则必须指定该参数，否则`Dense`层的输出维度无法自动推断。

**输入shape**

形如（samples，sequence_length）的2D张量

**输出shape**

形如(samples, sequence_length, output_dim)的3D张量

**例子**

```python
model = Sequential()
model.add(Embedding(1000, 64, input_length=10))
# 该模型以整数矩阵(batch,input_length)
# 输入中最大的整数(即单词索引)应不大于999
# model.output_shape == (None, 10, 64)

input_array = np.random.randint(1000, size=(32, 10))

model.compile('rmsprop', 'mse')
output_array = model.predict(input_array)
assert output_array.shape == (32, 10, 64)
```

### 参考

[来自《Keras中文文档》](http://keras-cn.readthedocs.io/en/latest/)