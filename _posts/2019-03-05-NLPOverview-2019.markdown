---
layout:     post
title:      "自然语言处理综述(一)"
subtitle:   "nlp overview"
date:       2019-03-05 10:00:00
author:     "Wwt"
header-img: "img/NLPoverview_1/bg.jpg"
catalog: true
tags:   
    - 词向量
    - 卷积神经网络
    - 循环神经网络

---
### 简介

自然语言处理（NLP）是指对人类语言进行自动分析和表示的计算技术，这种技术由一系列理论驱动。NLP研究 从打孔纸带和批处理的时代就开始发展，那时候分析一个句子需要多达7分钟的时间。到了现在谷歌等的时代，数百万网页可以在不到一秒钟内处理完成。NLP辅助计算机执行了大量自然语言相关的任务，如句子结构解析、词形标注、机器翻译和对话系统等。

深度学习架构和算法为计算机视觉与传统模式识别领域带来了巨大进展。紧跟这一趋势，现在的NLP研究越来越多地使用了新深度学习方法。之前数十年，用于解决NLP问题的机器学习方法一般都基于浅层模型（如SVM和Logistic回归），这些模型都在非常高维和稀疏的特征上取得了不错结果。这一趋势依赖于词嵌入和深度学习方法的成功。深度学习是多级自动特征表示学习成为可能。而基于传统机器学习的NLP系统严重依赖手动设计的特征，它们耗时，且不完备。

Ronan Collobert等人在2011年的研究《Natural Language Processing (Almost) from Scratch 》展示了在多个NLP任务上优于当时最优方法的简单深度学习框架，例如命名实体识别（NER）、语义角色标注（SRL）和词性标注。本文综述了用于自然语言任务的主要深度学习方法，如卷积神经网络、循环神经网络和递归神经网络。

### 分布式表征

基于统计的NLP已经成为建模复杂自然语言任务的首要选择。然而在它刚兴起的时候，基于统计的 NLP 经常遭受到维度灾难，尤其是在学习语言模型的联合概率函数时。这为能在低维空间中构建学习分布式词的表征方法提供了动力，这种想法也就导致了词嵌入方法的诞生。

第一种在低维空间中学习密集型的分布式词表征是 Yoshua Bengio 等人在 2003 年提出的 A Neural Probabilistic Language Model，这是一种基于学习而对抗维度灾难的优美想法。

### 词嵌入

如下图 1所示，分布式向量或词嵌入向量基本上遵循分布式假设，即具有相似语义的词也倾向于具有相似的上下文词，因此这些词向量尝试捕获邻近词的特征。分布式词向量的主要优点在于它们能捕获单词之间的相似性，使用余弦相似性等度量方法评估词向量之间的相似性也成为可能。

词嵌入常用于深度学习中的第一个数据预处理阶段，一般我们可以在大型无标注文本语料库中最优化损失函数，从而获得预训练的词嵌入向量。例如基于上下文预测具体词（Mikolov et al., 2013b, a）的方法，它能学习包含了一般句法和语义的词向量。这些词嵌入方法目前已经被证明能高效捕捉上下文相似性，并且由于它们的维度非常小，因此在计算核心 NLP 任务是非常快速与高效的。

![1](/img/NLPoverview_1/1.jpg)

多年以来，构建这种词嵌入向量的模型一般是浅层神经网络，并没有必要使用深层神经网络构建更好的词嵌入向量。不过基于深度学习的 NLP 模型常使用这些词嵌入表示短语甚至句子，这实际上是传统基于词统计模型和基于深度学习模型的主要差别。目前词嵌入已经是 NLP 任务的标配，大多数 NLP 任务的顶尖结果都需要借助它的能力。

本身词嵌入就能直接用于搜索近义词或者做词义的类推，而下游的情感分类、机器翻译、语言建模等任务都能使用词嵌入编码词层面的信息。最近比较流行的预训练语言模型其实也参考了词嵌入的想法，只不过预训练语言模型在词嵌入的基础上进一步能编码句子层面的语义信息。总的而言，词嵌入的广泛使用早已体现在众多文献中，它的重要性也得到一致的认可。

分布式表示（词嵌入）主要通过上下文或者词的「语境」来学习本身该如何表达。上个世纪 90 年代，就有一些研究（Elman, 1991）标志着分布式语义已经起步，后来的一些发展也都是对这些早期工作的修正。此外，这些早期研究还启发了隐狄利克雷分配等主题建模（Blei et al., 2003）方法和语言建模（Bengio et al., 2003）方法。

在 2003 年，Bengio 等人提出了一种神经语言模型，它可以学习单词的分布式表征。他们认为这些词表征一旦使用词序列的联合分布构建句子表征，那么就能构建指数级的语义近邻句。反过来，这种方法也能帮助词嵌入的泛化，因为未见过的句子现在可以通过近义词而得到足够多的信息。

![2](/img/NLPoverview_1/2.jpg)

Collobert 和 Weston(2008) 展示了第一个能有效利用预训练词嵌入的研究工作，他们提出的神经网络架构构成了当前很多方法的基础。这一项研究工作还率先将词嵌入作为 NLP 任务的高效工具，不过词嵌入真正走向 NLP 主流还是 Mikolov 等人在 2013 年做出的研究《Distributed Representations of Words and Phrases and their Compositionality》。

Mikolov 等研究者在这篇论文中提出了连续词袋模型（CBOW）和 Skip-Gram 模型，这两种方法都能学习高质量的分布式词表征。此外，令这两种方法受到极大关注的是另一种附加属性：语义合成性，即两个词向量相加得到的结果是语义相加的词，例如「man」+「royal」=「king」。这种语义合成性的理论依据最近已经由 Gittens et al. (2017) 给出，他们表示只有保证某些特定的假设才能满足语义合成性，例如词需要在嵌入空间中处于均匀分布。

Pennington et al. (2014) 提出了另一个非常出名的词嵌入方法 Glove，它基本上是一种基于词统计的模型。在某些情况下，CBOW 和 Skip-Gram 采用的交叉熵损失函数有劣势。因此 GloVe 采用了平方损失，它令词向量拟合预先基于整个数据集计算得到的全局统计信息，从而学习高效的词词表征。

一般 Glove 模型会先对单词计数进行归一化，并通过对数平滑来最终得到词共现矩阵，这个词共现矩阵就表示全局的统计信息。这个矩阵随后可以通过矩阵分解得到低维的词表征，这一过程可以通过最小化重构损失来获得。下面将具体介绍目前仍然广泛使用的 CBOW 和 Skip-Gram 两种Word2Vec方法（Mikolov et al., 2013）。

### Word2Vec

可以说 Mikolov 等人彻底变革了词嵌入，尤其是他们提出的 CBOW 和 Skip-Gram 模型。CBOW 会在给定上下文词的情况下计算目标词（或中心词）的条件概率，其中上下文词的选取范围通过窗口大小 k 决定。而 Skip-Gram的做法正好与CBOW相反，它在给定目标词或中心词的情况下预测上下文词。一般上下文词都会以目标词为中心对称地分布在两边，且在窗口内的词与中心词的距离都相等。也就是说不能因为某个上下文词离中心词比较远，就认为它对中心词的作用比较弱。

在无监督的设定中，词嵌入的维度可以直接影响到预测的准确度。一般随着词嵌入维度的增加，预测的准确度也会增加，直到准确率收敛到某个点。一般这样的收敛点可以认为是最佳的词嵌入维度，因为它在不影响准确率的情况下最精简。通常情况下，我们使用的词嵌入维度可以是 128、256、300、500 等，相比于几十万的词汇库大小已经是很小的维度了。

下面我们可以考虑 CBOW 的简化版，上下文只考虑离中心词最近的一个单词，这基本上就是二元语言模型的翻版。

![3](/img/NLPoverview_1/3.jpg)

如图 43所示，CBOW 模型就是一个简单的全连接神经网络，它只有一个隐藏层。输入层是上下文词的 one-hot 向量，它有 V 个神经元（词汇量），而中间的隐藏层只有 N 个神经元，N 是要远远小于 V 的。最后的输出层是所有词上的一个 Softmax 函数。层级之间的权重矩阵分别是 $V*N$ 阶的 $W $和$ N*V$ 阶的 $W'$，词汇表中的每一个词最终会表征为两个向量：$v_c$ 和 $v_w$，它们分别对应上下文词表征和目标词表征。若输入的是词表中第 k 个词，那么我们有：

$$v_c = W_{(k,.)} and v_w=W^{，}_{(.,k)}$$

总体而言，在给定上下文词 c 作为输入的情况下，对于任意词$ w_i $有：

$$P(\frac{w_i}{c})=y_i=\frac{e^{u_i}}{\sum_{i=1}^V}e^{u_i} where u_i=v^T_{w_i}*V_c$$

参数$ θ={V_w, V_c}$ 都是通过定义目标函数而学习到的，一般目标函数可以定义为对数似然函数，且通过计算以下梯度更新权重：

$$l(\theta)=\sum_{w\in Vocabulary}log(P(\frac{w}{c})​$$

$$\frac{\vartheta(\theta)}{\vartheta V_w}=V_c(1-P(\frac{w}{c}))$$

在更广泛的 CBOW 模型中，所有上下文词的 one-hot 向量都会同时作为输入，即：

$$h=W^T(x_1+x_2+...+x_c)$$

词嵌入的一个局限是它们无法表示短语（Mikolov et al., 2013），即两个词或多个词的组合并不表示对应的短语意义，例如「人民」+「大学」并不能组合成「人民大学」。Mikolov 提出的一种解决办法是基于词共现识别这些短语，并为它们单独地学一些词嵌入向量，而 Rie Johnson 等研究者在 15 年更是提出直接从无监督数据中学习 n-gram 词嵌入。

另一种局限性在于学习的词嵌入仅基于周围词的小窗口，有时候「good」和「bad」几乎有相同的词嵌入，这对情感分析等下游任务很不友好。有时候这些相似的词嵌入有正好相反的情感，这对于需要区别情感的下游任务简直是个灾难，它甚至比用 One-hot 向量的表征方法还要有更差的性能。Duyu Tang（2014）等人通过提出特定情感词嵌入（SSWE）来解决这个问题，他们在学习嵌入时将损失函数中的监督情感纳入其中。

一个比较重要的观点是，词嵌入应该高度依赖于他们要使用的领域。Labutov 和 Lipson(2013) 提出了一种用于特定任务的词嵌入，他们会重新训练词嵌入，因此将词嵌入与将要进行的下游任务相匹配，不过这种方法对计算力的需求比较大。而 Mikolov 等人尝试使用负采样的方法来解决这个问题，负采样仅仅只是基于频率对负样本进行采样，这个过程直接在训练中进行。

此外，传统的词嵌入算法为每个词分配不同的向量，这使得其不能解释多义词。在最近的一项工作中，Upadhyay 等人 (2017) 提出了一种新方法来解决这个问题，他们利用多语平行数据来学习多语义词嵌入。例如英语的「bank」在翻译到法语时有两种不同的词：banc 和 banque，它们分别表示金融和地理意义，而多语言的分布信息能帮助词嵌入解决一词多义的问题。

### 卷积神经网络

随着词嵌入的流行及其在分布式空间中展现出的强大表征能力，我们需要一种高效的特征函数，以从词序列或 n-grams 中抽取高级语义信息。随后这些抽象的语义信息能用于许多 NLP 任务，如情感分析、自动摘要、机器翻译和问答系统等。卷积神经网络（CNN）因为其在计算机视觉中的有效性而被引入到自然语言处理中，实践证明它也非常适合序列建模。

![4](/img/NLPoverview_1/4.jpg)

使用 CNN 进行句子建模可以追溯到 Collobert 和 Weston (2008) 的研究，他们使用多任务学习为不同的 NLP 任务输出多个预测，如词性标注、语块分割、命名实体标签和语义相似词等。其中查找表可以将每一个词转换为一个用户自定义维度的向量。因此通过查找表，n 个词的输入序列$ {s_1，s_2，... s_n } $能转换为一系列词向量 ${w_s1, w_s2,... w_sn}$，这就是图 4所示的输入。

这可以被认为是简单的词嵌入方法，其中权重都是通过网络来学习的。在 Collobert 2011 年的研究中，他扩展了以前的研究，并提出了一种基于 CNN 的通用框架来解决大量 NLP 任务，这两个工作都令 NLP 研究者尝试在各种任务中普及 CNN 架构。

CNN 具有从输入句子抽取 n-gram 特征的能力，因此它能为下游任务提供具有句子层面信息的隐藏语义表征。下面简单描述了一个基于 CNN 的句子卷积网络到底是如何处理的。

#### 基础CNN

1.序列建模

对于每一个句子，$w_i∈R^d$ 表示句子中第 i 个词的词嵌入向量，其中 d 表示词嵌入的维度。给定有 n 个词的句子，句子能表示为词嵌入矩阵 $W∈R^n×d$。下图展示了将这样一个句子作为输入送到 CNN 架构中。

![5](/img/NLPoverview_1/5.jpg)

若令$ w_i:i+j $表示 $w_i, w_i+1,...w_j$向量的拼接，那么卷积就可以直接在这个词嵌入输入层做运算。卷积包含 d 个通道的卷积核$ k∈R^hd$，它可以应用到窗口为 h 个词的序列上，并生成新的特征。例如，$c_i ​$即使用卷积核在词嵌入矩阵上得到的激活结果：

$$c_i=f(w_{i:i+h-1}.K^T+b)$$

若 b 是偏置项，f 是非线性激活函数，例如双曲正切函数。使用相同的权重将滤波器 k 应用于所有可能的窗口，以创建特征图。

$$c=[c_1,c_2,...,c_{n-h+1}]$$

在卷积神经网络中，大量不同宽度的卷积滤波器（也叫做卷积核，通常有几百个）在整个词嵌入矩阵上滑动。每个卷积核提取一个特定的 n-gram 模式。卷积层之后通常是最大池化策略 $c=max{c}​$，该策略通过对每个滤波器应用最大运算来对输入进行二次采样。使用这个策略有两大原因。

首先，最大池化提供固定长度的输出，这是分类所需的。因此，不管滤波器的大小如何，最大池化总是将输入映射到输出的固定维度上。其次，它在降低输出维度的同时保持了整个句子中最显著的 n-gram 特征。这是通过平移不变的方式实现的，每个滤波器都能从句子的任何地方提取特定的特征（如，否定），并加到句子的最终表示中。

词嵌入可以随机初始化，也可以在大型未标记语料库上进行预训练。第二种方法有时对性能提高更有利，特别是当标记数据有限时。卷积层和最大池化的这种组合通常被堆叠起来，以构建深度 CNN 网络。这些顺序卷积有助于对句子的建模，以获得包含丰富语义信息的真正抽象表征。卷积核通过更深的卷积覆盖了句子的大部分，直到完全覆盖并构建了句子特征的总体概括。

2.窗口方法

上述架构将完整句子建模为句子表征。然而，许多 NLP 任务（如命名实体识别，词性标注和语义角色标注）需要基于字的预测。为了使 CNN 适应这样的任务，需要使用窗口方法，其假定单词的标签主要取决于其相邻单词。因此，对于每个单词，存在固定大小的窗口，窗口内的子句都在处理的范围内。如前所述，独立的 CNN 应用于该子句，并且预测结果归因于窗口中心的单词。按照这个方法，Poira 等人（2016）采用多级深度 CNN 来标记句子中的每个单词为 aspect 或 non-aspect。结合一些语言模式，它们的集成分类器在 aspect 检测方面表现很好。

词级分类的最终目的通常是为整个句子分配一系列的标签。在这样的情况下，有时会采用结构化预测技术来更好地捕获相邻分类标签间的关系，最终生成连贯标签序列，从而给整个句子提供最大分数。

为了获得更大的上下文范围，经典窗口方法通常与时延神经网络（TDNN）相结合。这种方法中，可以在整个序列的所有窗口上进行卷积。通过定义特定宽度的内核，卷积通常会受到约束。因此，相较于经典窗口方法（只考虑要标记单词周围窗口中的单词），TDNN 会同时考虑句子中的所有单词窗口。TDNN 有时也能像 CNN 架构一样堆叠，以提取较低层的局部特征和较高层的总体特征。

#### 应用

Kim 探讨了使用上述架构进行各种句子分类任务，包括情感、主观性和问题类型分类，结果很有竞争力。因其简单有效的特点，这种方法很快被研究者接受。在针对特定任务进行训练之后，随机初始化的卷积内核成为特定 n-gram 的特征检测器，这些检测器对于目标任务非常有用。但是这个网络有很多缺点，最主要的一点是 CNN 没有办法构建长距离依存关系。

Kalchbrenner 等人的研究在一定程度上解决了上述问题。他们发表了一篇著名的论文，提出了一种用于句子语义建模的动态卷积神经网络（DCNN）。他们提出了动态 k-max 池化策略，即给定一个序列 p，选择 k 种最有效的特征。选择时保留特征的顺序，但对其特定位置不敏感。在 TDNN 的基础上，他们增加了动态 k-max 池化策略来创建句子模型。这种结合使得具有较小宽度的滤波器能跨越输入句子的长范围，从而在整个句子中积累重要信息。在下图中，高阶特征具有高度可变的范围，可能是较短且集中，或者整体的，和输入句子一样长。他们将模型应用到多种任务中，包括情感预测和问题类型分类等，取得了显著的成果。总的来说，这项工作在尝试为上下文语义建模的同时，对单个卷积核的范围进行了注释，并提出了一种扩展其范围的方法。

![6](/img/NLPoverview_1/6.jpg)

情感分类等任务还需要有效地抽取 aspect 与其情感极性（Mukherjee and Liu, 2012）。Ruder 等人（2016）还将 CNN 应用到了这类任务，他们将 aspect 向量与词嵌入向量拼接以作为输入，并获得了很好的效果。CNN 建模的方法一般因文本的长短而异，在较长文本上的效果比较短文本上好。Wang et al. (2015) 提出利用 CNN 建模短文本的表示，但是因为缺少可用的上下文信息，他们需要额外的工作来创建有意义的表征。因此作者提出了语义聚类，其引入了多尺度语义单元以作为短文本的外部知识。最后 CNN 组合这些单元以形成整体表示。

CNN 还广泛用于其它任务，例如 Denil et al. (2014) 利用 DCNN 将构成句子的单词含义映射到文本摘要中。其中 DCNN 同时在句子级别和文档级别学习卷积核，这些卷积核会分层学习并捕获不同水平的特征，因此 DCNN 最后能将底层的词汇特征组合为高级语义概念。

此外，CNN 也适用于需要语义匹配的 NLP 任务。例如我们可以利用 CNN 将查询与文档映射到固定维度的语义空间，并根据余弦相似性对与特定查询相关的文档进行排序。在 QA 领域，CNN 也能度量问题和实体之间的语义相似性，并借此搜索与问题相关的回答。机器翻译等任务需要使用序列信息和长期依赖关系，因此从结构上来说，这种任务不太适合 CNN。但是因为 CNN 的高效计算，还是有很多研究者尝试使用 CNN 解决机器翻译问题。

总体而言，CNN 在上下文窗口中挖掘语义信息非常有效，然而它们是一种需要大量数据训练大量参数的模型。因此在数据量不够的情况下，CNN 的效果会显著降低。CNN 另一个长期存在的问题是它们无法对长距离上下文信息进行建模并保留序列信息，其它如递归神经网络等在这方面有更好的表现。

### RNN模型

#### 1.简单RNN

在 NLP 中，RNN 主要基于 Elman 网络，最初是三层网络。图 9 展示了一个较通用的 RNN，它按时间展开以适应整个序列。图中$ x_t ​$作为网络在时间步 t 处的输入，$s_t ​$表示在时间步 t 处的隐藏状态。$s_t ​$的计算公式如下：

$$s_t=f(Ux_t+Ws_{t-1})$$

因此，$s_t ​$的计算基于当前输入和之前时间步的隐藏状态。函数 f 用来做非线性变换，如 tanh、ReLU，U、V、W 表示在不同时间上共享的权重。在 NLP 任务中，$x_t ​$通常由 one-hot 编码或嵌入组成。它们还可以是文本内容的抽象表征。$o_t ​$表示网络输出，通常也是非线性的，尤其是当网络下游还有其他层的时候。

RNN 的隐藏状态通常被认为是其最重要的元素。如前所述，它被视为 RNN 的记忆元素，从其他时间步中累积信息。但是，在实践中，这些简单 RNN 网络会遇到梯度消失问题，使学习和调整网络之前层的参数变得非常困难。

#### 2.长短期记忆网络（LSTM）

LSTM 比简单 RNN 多了『遗忘』门，其独特机制帮助该网络克服了梯度消失和梯度爆炸问题。

![8](/img/NLPoverview_1/7.jpg)

与原版 RNN 不同，LSTM 允许误差通过无限数量的时间步进行反向传播。它包含三个门：输入门、遗忘门和输出门，并通过结合这三个门来计算隐藏状态，如下面的公式所示:

$$\begin {align} x&=\begin{bmatrix}h_{t-1} \\x_t \end{bmatrix} \\ f_t&=\sigma(W_fx+b_f)  \\i_t&=\sigma(w_ix+b)\\o_t&=\sigma(W_o+b_o)\\ c_t&=f_t \bigodot c_{t-1} +i_t\bigodot tanh(W_cX+b_c)\\h_t&=o_t\bigodot tanh(c_t)&\end{align}​$$

#### 3.门控循环单元（GRU）

另一个门控 RNN 变体是 GRU，复杂度更小，其在大部分任务中的实验性能与 LSTM 类似。GRU 包括两个门：重置门和更新门，并像没有记忆单元的 LSTM 那样处理信息流。因此，GRU 不加控制地暴露出所有的隐藏内容。由于 GRU 的复杂度较低，它比 LSTM 更加高效。其工作原理如下：

$$\begin {align} z&=\sigma(U_z*x_t+W_zh_{t-1})\\r&=\sigma(U_rx_t+W_rh_{t-1})\\ s_t&=tanh(U_z*x_t+W_s(h_{t-1}\bigodot r))\\h_t&=(1-z)\bigodot s_t+z\bigodot h_{t-1} \end{align}$$

研究者通常面临选择合适门控 RNN 的难题，这个问题同样困扰 NLP 领域开发者。纵观历史，大部分对 RNN 变体的选择都是启发式的。《Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling》对前面三种 RNN 变体进行了对比评估，不过评估不是在 NLP 任务上进行的，而是复调音乐建模和语音信号建模相关任务。他们的评估结果明确表明门控单元（LSTM 和 GRU）优于传统的简单 RNN（实验中用的是 tanh 激活函数）。

#### 应用

**1.用于单词级别分类任务的 RNN**

之前，RNN 经常出现在单词级别的分类任务中。其中的很多应用到现在仍然是所在任务中的最优结果。论文《Neural Architectures for Named Entity Recognition》提出 LSTM+CRF 架构。它使用双向 LSTM 解决命名实体识别问题，该网络捕捉目标单词周围的任意长度上下文信息（缓解了固定窗口大小的约束），从而生成两个固定大小的向量，再在向量之上构建另一个全连接层。最终的实体标注部分使用的是 CRF 层。

RNN 在语言建模任务上也极大地改善了基于 count statistics 的传统方法。该领域的开创性研究是 Alex Graves 2013 年的研究《Generating Sequences With Recurrent Neural Networks》，介绍了 RNN 能够有效建模具备长距离语境结构的复杂序列。该研究首次将 RNN 的应用扩展到 NLP 以外。之后，Sundermeyer 等人的研究《From Feedforward to Recurrent LSTM Neural Networks for Language Modeling》对比了在单词预测任务中用 RNN 替换前馈神经网络获得的收益。该研究提出一种典型的神经网络层级架构，其中前馈神经网络比基于 count 的传统语言模型有较大改善，RNN 效果更好，LSTM 的效果又有改进。该研究的一个重点是他们的结论可应用于多种其他任务，如统计机器翻译。

**2.用于句子级别分类任务的 RNN**

Xin Wang 等人 2015 年的研究《Predicting Polarities of Tweets by Composing Word Embeddings with Long Short-Term Memory》提出使用 LSTM 编码整篇推文（tweet），用 LSTM 的隐藏状态预测情感极性。这种简单的策略被证明与 Nal Kalchbrenner 等人 2014 年的研究《A Convolutional Neural Network for Modelling Sentences》提出的较复杂 DCNN 结构性能相当，DCNN 旨在使 CNN 模型具备捕捉长期依赖的能力。在一个研究否定词组（negation phrase）的特殊案例中，Xin Wang 等人展示了 LSTM 门的动态可以捕捉单词 not 的反转效应。

与 CNN 类似，RNN 的隐藏状态也可用于文本之间的语义匹配。在对话系统中，Lowe 等人 2015 年的研究《The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems》提出用 Dual-LSTM 匹配信息和候选回复，Dual-LSTM 将二者编码为固定大小的向量，然后衡量它们的内积用于对候选回复进行排序。

**3.用于生成语言的 RNN**

NLP 领域中的一大难题是生成自然语言，而这是 RNN 另一个恰当的应用。基于文本或视觉数据，深度 LSTM 在机器翻译、图像字幕生成等任务中能够生成合理的任务特定文本。在这些案例中，RNN 作为解码器。

在 Ilya Sutskever 等人 2014 年的研究《Sequence to Sequence Learning with Neural Networks》中，作者提出了一种通用深度 LSTM 编码器-解码器框架，可以实现序列之间的映射。使用一个 LSTM 将源序列编码为定长向量，源序列可以是机器翻译任务中的源语言、问答任务中的问题或对话系统中的待回复信息。然后将该向量作为另一个 LSTM（即解码器）的初始状态。在推断过程中，解码器逐个生成 token，同时使用最后生成的 token 更新隐藏状态。束搜索通常用于近似最优序列。

该研究使用了一个 4 层 LSTM 在机器翻译任务上进行端到端实验，结果颇具竞争力。《A Neural Conversational Model》使用了同样的编码器-解码器框架来生成开放域的有趣回复。使用 LSTM 解码器处理额外信号从而获取某种效果现在是一种普遍做法了。《A Persona-Based Neural Conversation Model》提出用解码器处理恒定人物向量（constant persona vector），该向量捕捉单个说话人的个人信息。在上述案例中，语言生成主要基于表示文本输入的语义向量。类似的框架还可用于基于图像的语言生成，使用视觉特征作为 LSTM 解码器的初始状态（图 12）。

视觉 QA 是另一种任务，需要基于文本和视觉线索生成语言。2015 年的论文《Ask Your Neurons: A Neural-based Approach to Answering Questions about Images》是首个提供端到端深度学习解决方案的研究，他们使用 CNN 建模输入图像、LSTM 建模文本，从而预测答案（一组单词）。

### 参考

>[Modern Deep Learning Techniques Applied to Natural Language Processing](https://nlpoverview.com/index.html)
>
>[万字长文概述NLP中的深度学习技术](https://zhuanlan.zhihu.com/p/57979184)





