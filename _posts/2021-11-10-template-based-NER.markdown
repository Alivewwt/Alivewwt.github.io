---
layout:     post
title:      "基于模板的命名实体识别"
subtitle:   "named entity recognition"
date:       2021-11-10 10:00:00
author:     "Wwt"
header-img: "img/template-ner/bg.png"
catalog: true
tags:   
    - NLP
---
>论文《Template-Based Named Entity Recognition Using BART》
>
>地址：https://arxiv.org/abs/2004.01970


### 介绍

命名实体识别是一项基础的NER任务，最近的研究热点集中在few-shot NER任务上 ，目的是将资源丰富领域的不同标签迁移到低资源领域上。现在方法是基于相似性进行度量。然而这种方法不能完全利用NER模型参数进行知识迁移。

为了解决这个问题，我们提出了一个基于模版的NER方法，将NER视为序列到序列框架中语言模型的排序问题。其中，源序列是由候选命名实体span填充的原始句子，语句模版则作为目标序列。对于推理，模型需要根据相应的模版分数对每个候选span进行分类。实验结果表明我们的方法在低资源的NER任务上取得了很好的效果。

### 背景

基于神经网络的NER模型需要大量的训练数据，这些海量数据在某些领域可以容易获取到，如新闻，但是大多数在其他领域中却很少。理想情况下，从资源丰富的新闻领域进行知识转移是可行的，这样模型就可以基于一些标注样例迁移到目标领域中。但是，在实际应用过程中会有一个问题就是：实体类别在不同领域中可能不同，如下图所示。在新闻领域需要识别地点和人物，但在电影领域需要识别演员和电影名。Softmax和CRF层在训练和测试时使用的标签需要一致。因此，给定一个新目标领域，输出层必须要经过源域和目标域一致的训练进行调整，代价是十分昂贵的。

![1](/img/template-ner/1.png)

最近的工作尝试通过距离度量来研究少样本NER，主要思想是基于源领域的样本训练一个相似度函数，然后在目标领域中利用相似度函数作为few-shot NER的最近邻准则。
上述方法降低了领域的适应性成本，特别是对于那些目标域数据量较多的场景。另外，该方法的领域适应能力也受到两个方面的限制。一方面，目标域的标记实例用于启发式最近邻搜索找到最佳参数，而不是更新NER模型的参数，虽然成本较低，但这些无法改善跨域的实例神经网络表示。另一方面，这些方法依赖源域和目标域的相似文本模式。当目标域的写作风格和源域不匹配时，这种假设会影响模型性能。

为了解决以上问题，我们提出了基于模板方法使用预训练语言模型生成序列来挖掘在few shot 学习任务上的潜在能力。如下图所示，BART使用由相应标记实体填充预定义的模板进行微调。举个例子，我们定义类似的模板"<candidate_span> is a <entity_type> entity", 其中，<entity_type> 能够被"person"和“location”等实体类型填充。

![2](/img/template-ner/2.png)

例如，句子"ACL will be held in Bangkok" ，"Bangkok"是一个标注标签"location"，我们可以使用模板“Bangkok is a location entity”作为输入句子的解码输出来训练BART。在非实体词方面，我们使用模板"<cadidate_span> is not a named entity"，因此也可以对输出负例序列进行采样。在推理过程中，我们遍历句子中所有可能的span作为候选实体，然后根据BART的得分对它们进行实体识别。

我们提出的方法具有以下三个优点，预训练模型有很好的泛化能力，网络可以有效地利用标记的新实例在新领域上进行微调；其次，即使目标领域和源领域风格相差很大，我们的方法也更健壮；最后，与传统方法相比（带有softmax/crf的预训练模型），我们可以在不改变输出层的情况下，适用于任何新类别的命名实体识别。

### 方法

我们将NER视作seq2seq框架下的一个语言模型的重排问题。输入到模型的文本是$X={x_1,x_2,...x_n}$，目标序列$T_{yk,x_{i:j}}={t_1,...,t_m}$是由候选文本$x_{i:j}$和实体类型$y_k$填充成的。

#### 模板生成

我们人工构建模板，一个槽位是候选span，一个槽位是实体类型标签。设置映射函数将$L={l_1,l_2,...l_{\mid L \mid}}$映射到自然语言集和$Y={y_1,...y_{\mid L \mid}}$。定义模板$T^{+}_{yk}$(例如，<candidate_span>是地点实体)。另外，针对非实体类型，我们创建一个非实体模板$T^{-}$。通过这种方式，我们获得了一系列模板$T=[T^{+}_{y_1},...,T^{+}_{y \mid L\mid}, T^{-}]$,如上图中c所示。

#### 推理

我们首先遍历句子${x_1,...x_n}$的所有span，将它们填充进准备好的模板里。我们严格的将n-gram长度设置为1到8，因此每个句子创建了$8n$数量的模板,然后我们使用预训练模型进行微调，给每个模板进行打分，公式如下：


$$
f(T_{yk,x_{i:j}})=\sum^m_{c=1}log p(t_c \mid t_{1:c-1},X)
$$


通过预训练模型计算每个实体$f(T^+_{yk,x_{i:j}})$和非实体$f(T^-_{x_{i:j}})$的得分。然后我们将最大得分实体类型$x_{i:j}$赋给该文本，在本文中主要使用了BART预训练语言模型。

数据集里不包括嵌套实体，如果两个span存在文本重叠，并被分配了不同的标签，我们会选择得分最高的标签。例如，给定句子"ACL will be held in Bangkok"， “in Bangkok” 和"Bangkok"能够分别被分配到"ORG"和"LOC"，在这种情况下，我们会比较$f(T^+_{ORG},'in Bangkok')$和$f(T^{+}_{LOC,'Bangkok'})$的分数，选择分数最大的作为最终标签。

#### 训练

标注实体在训练期间创建模板，假设$x_{i:j}$的实体类型是$y_k$，我们将文本$x_{i:j}$和对应的实体类型$y_k$填充到$T^{+}$来创建目标句子$T^{+}_{y_k,x_{i:j}}$。同样的，如果$x_{i:j}$的实体类型是空，则将$x_{i:j}$填充到$T^-$得到目标句$T^-{x_{i:j}}$。我们使用训练集中的所有标注实体来构建$(X,T^+)$对，另外通过抽样创建负样本$(X,T^-)$。负样本的数量是正样本的1.5倍。

![3](/img/template-ner/3.png)

#### 迁移学习

给定一个新领域$P$的少量样本，标签集合可能与用于训练NER模型的不同。因此，我们用新领域标签集填充模板用于训练和测试，其余模型和算法保持不变。特别是，给定少量$(XP,TP)$，我们使用上述方法为低资源创建序列，并在资源丰富的数据集上微调NER模型。这个过程成本低，但可以有效地传递标签知识。因为我们方法的输出是一个自然句子而不是特定的标签，资源丰富和匮乏的标签词汇都是预训练模型词汇表$(VR，VP \notin V)$的子集。这使得我们的方法可以利用标签如"人物"和"演员"，“位置”和"城市"等相关性，来增强跨域迁移学习的效果。

### 实验效果

<img src="/img/template-ner/4.png" alt="4" style="zoom:50%;" />

我们在CONLL03的数据集进行了NER实验，与目前先进的方法进行对比，从上表中可以看出，基于BERT的序列标注是一个强基线模型，F1 score达到了91.73%。尽管基于模板的BART模型是为小样本NER任务设计的，但在资源丰富的数据集上也同样具有竞争力，在召回率上，比BERT领先了1.8%。这表明我们的方法在命名实体方面更有效，而且可以选择不相关的span。另外，尽管基于BART的序列标注和使用模板的BART都用到了BART解码器，但是两者之间的性能却有着很大的差距，后者在F1 的得分上高于前者1.3%，这表明BART在序列标注任务上不具有竞争力，这可能是因为基于Seq2Seq的去噪自动编码器训练性质导致的，与BERT的掩码语言建模不同。

为了探索模板是否相互补充，我们使用表1中的3个模板进行训练，然后采用投票方法来集成这三个模型。集成模型的准确率提升了1.21%，表明了不同模板能够捕获不同类型的知识。最后，我们的集成模型的F1 score取得了92.55%，这在目前NER的任务上最佳的性能。

#### 小样本实验

<img src="/img/template-ner/5.png" alt="5" style="zoom:50%;" />

我们不使用源域数据集，从零开始训练模型，基于距离度量的方法不适合该方法。与传统的基于BERT序列标注方法相比，我们可以更好的利用少样本数据。特别地，在MIT restaurant的数据集上，只有20个实体实例的性能超过了BERT使用100个实体实例，取得了57.1%的f1 score。

从上表中，我们可以看出在三个数据集上，当训练实例数量较少时，few shot方法优于基于BERT和BART的序列标注方法。然而，随着训练实例数量增加，few shot方法的优势会降低。当实例增加到500，BERT优于现在所有方法。我们的方法在10个和500个实例中均有效，均优于BERT和基线的few shot 方法。

 