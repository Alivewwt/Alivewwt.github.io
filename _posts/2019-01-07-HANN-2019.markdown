---
layout:     post
title:      "基于监督和层次的注意力机制模型"
subtitle:   "层次注意力"
date:       2019-01-07 10:00:00
author:     "Wwt"
header-img: "img/HANN/bg.png"
catalog: true
tags:   
    - 层次注意力
    - 监督注意力
    - 事件抽取

---
文本分类是一项基础的NLP任务，在主题分类、情感分析、垃圾邮件检测等应用上有广泛的应用。目标是给每篇文档分配一个类别标签。本文针对文本分类任务提出了一个层次化attention机制模型Hierarchical Attention Networks for Document Classification(HAN)，有两个显著的特点：

- 采用“词-句子-文章”的层次化结构来表示一篇文本。
- 该模型有两个层次的attention机制，分别存在于词层次（Word level）和句子层次（Sentence level）。从而使该模型具有对文本中重要性不同的句子和词给予不同的“注意力”的能力。

HAN的灵感来源于人在阅读文章的时候，不同的词和句子对于人理解文章信息有不同的影响。因为，词和句子的重要性是和上下文息息相关的，即使是相同的词和句子，在不同的上下文中重要性也不一样。人在阅读一篇文章时，对文章不同的内容有着不同的注意度。

因此，本文在attention机制的基础上，联想到document是一个层次化的结构，提出用词相邻表示句子向量，再由句子向量表示document向量，并且在词层次和句子层次分别引入attention操作的模型。实验表明，HAN模型相比它同时期的其他模型，实验指标大幅提升。

### 模型介绍

HAN的模型结构如下所示，它包含一个词序列编码器，一个word level 的attention 层，一个句子序列编码器，一个sentence level的attention层。

![1](/img/HANN/1.jpg)

#### 基于GRU的词序列编码器

GRU是RNN的一个变种，使用门机制来记录序列当前的状态。隐藏层有两个门(gate)，重置门(reset gate)$r_t$和更新门（update gate）$z_t$。这两个门一起来控制当前状态有多少信息要更新。在时刻$t$,隐藏层状态的计算公式：

$$h_t=(1-z_t) \odot h_{t-1}+z_t\odot\hat{h_t}$$

更新门(update gate)$z_t$是用来决定有多少过去的信息被保留，以及有多少新信息被加进来：

$$z_t= \sigma(W_zx_t+U_zh_{t-1}+b_z)$$

这里$x_t$是在时刻$t$输入的单词词向量，候选状态$\hat{h_t}$的计算方法和普通的RNN相似：
$$\hat{h_t}=tanh(W_hx_t+r_t\odot(U_hh_{t-1})+b_h)$$

重置门$r_t$决定有多少过去的信息作用于候选状态，如果$r_t$是0，即忘记之前的所有状态：

$$r_t=\sigma(W_rx_t+U_rh_{t-1}+b_r)$$

#### 层次化

##### 词编码器

由词序列组成$w_{it}，t\in[0,T]$组成的句子，首先把词转化成词向量，$x_{it}=W_ew_{it}$，然后用双向GRU网络，可以将正向和反向的上下文信息结合起来，获得隐藏层输出。

$$x_{it}=W_ew_{it},t\in[1,T]$$

$$\overrightarrow{h_{it}}=\overrightarrow{GRU}(x_{it})$$

$$\overleftarrow{h_{it}}=\overleftarrow{GRU}(x_{it})$$

对于一个给定的词语$w_{it}$，经过双向GRU之后，我们获得了一种新的表示：$h_{it}=[\overrightarrow{h_{it}},\overleftarrow{h_{it}}]$，$h_{it}$包含了$w_{it}$周围两个方向的信息。

##### 词级别的attention机制

attention机制的目的是要把一个句子中，对句子含义最重要，贡献最大的词语找出来。我们通过将$h_{it}$输入到一个单层的全连接层中得到结果$u_{it}$作为$h_{it}$的隐含表示。

$$u_{it}=tanh(W_wh_{it}+b_w)$$

为了衡量单词的重要性，我们用$u_{it}$和一个随机初始化的上下文向量$u_w$的相似度来表示，然后经过softmax操作获得了一个归一化的attention权重矩阵$\alpha_{(it)}$，代表句子$i$中第$t$个词的权重。

$$\alpha_{it}=\frac{exp(u_{it}^T)u_w}{\sum exp(u_{it}^Tu_w)}$$

得到了attention权重矩阵之后，句子向量$s_i$可以看做这些词向量的加权求和。这里的上下文向量$u_w$是在训练网络过程中学习获得的。我们可以把$u_w$当做一种询问的高级表示，比如“哪些词含有比较重要的信息？”

$$s_i=\sum\alpha_{it}h_{it}$$

##### 句子编码器

得到了句子向量表示$s_i$以后，可以用类似的办法获得文档向量：

$$\overrightarrow{h_i}=\overrightarrow{GRU}(s_i),i\in[1,L]$$

$$\overleftarrow{h_i}=\overleftarrow{GRU}(s_i),i\in[L,1]​$$

对应给定的句子$s_i​$，相应的句子表示

$$\ h_i=[\overrightarrow{h_i},\overleftarrow{h_i}]$$。

这样，可以包含两个方向的上下文信息的表示。

##### 句子级别的attention机制

和word level 的attention类似，对于document，也有一个句子级别的上下文向量$u_s$，来衡量一个句子相对于document的重要性。

$$u_i=tanh(W_sh_i+b_s)$$

$$\alpha_i=\frac{exp(u_i^Tu_s)}{\sum_texp(u_t^Tu_s)}$$

$$v=\sum_i\alpha_ih_i$$

在这里我们就获得了整篇文章的向量表示$v$，最后可以使用全连接的softmax层进行分类。

### 事件抽取领域的拓展

本文提出了一种基于层次化attention的文本分类模型，可以利用attention机制识别出一句话中比较重要的词语，利用重要的词语形成句子的表示，同样识别出重要的句子，利用重要句子表示来成整篇文本的表示。实验证明，该模型确实比基准模型获得了更好的效果，可视化分析也表明，该模型能很好地识别出重要的句子和词语。

基于上述思想，我们认为在文档级别的信息在句子级别的事件抽取领域中是非常重要的。因此我们提出文档级向量来增强双向RNN模型，称作DEEB-RNN。这个文档向量的来源就是通过层次和有监督的注意力机制来学习到文档向量，主要是采用次词级别的注意力关注句子的触发词以及句子级别的注意力关注含有触发词的句子。然后将学习到文档向量应用到另外一个双向RNN模型中来进行触发词抽取。

![2](/img/HANN/2.png)

模型如上图所示，主要包括基于词级别注意力机制的双向GRU，句子级别注意力机制的双向GRU。为了使触发词得到更多关注，我们对句子$$s_i$$构建了gold 词级别的注意力信号$\alpha_{i}^*$。所以在词级别，我们可以根据计算注意力的平方损失函数来有监督的训练模型。

$$E_w(\alpha^*,\alpha)=\sum^L_{i=1}\sum^T_{t=1}(\alpha^*_{it}-\alpha_{it})^2$$

同样的，我们认为含有触发词的句子应该比其它句子获得更多的注意。所以，我们对于文档$d$设计了一个gold 句子级别的注意力信号$\beta^*$,在句子级别，我们可以根据计算注意力的平方损失函数来有监督的训练模型。

$$E_s(\beta^*,\beta)=\sum^L_{i=1}(\beta^*_i-\beta_i)^2$$

在图中右半部分，我们使用了另一个双向RNN和全连接层来进行触发词识别和分类。具体地，给定一个文档$d$中的某个句子$s_j(j=1,2,3,...L)$，对每个词$$w_{jt}(t=1,2,3,...T)$$，我们将它的词向量$w_{jt}$，实体类型向量$e_{jt}$和对应的文档向量$d$拼接在一起，作为双向GRU的输入。

在模型DEEB-RNN中，上述的两个模块联合训练。在训练阶段，我们设计了一个联合损失函数。

$$\mathbb{J(\theta)}=\sum(J(y,o)+\lambda E_w(\alpha^*,\alpha)+\mu E_s(\beta^*,\beta))$$

#### 结论

文章设计了五个对比实验来证明有监督的层次注意力机制能够提高触发词抽取性能。

Bi-GRU是基本的事件抽取模型，没使用文档级别的向量。

DEEB-RNN模型使用了文档向量，没使用有监督机制来训练模型，超参数$\lambda$和$\mu$均为0。

DEEB-RNN1/2/3使用了gold注意力信号作为有监督信息。具体来说，DEEB-RNN1只使用了gold词级别的注意力信号$$（\lambda=1, \mu=0）$$，DEEB-RNN2只使用了gold 句子级别的注意力信号$$(\lambda=0,\mu=1)$$,DEEB-RNN3 使用了gold词级别和句子级别的注意力信号$$（\lambda=1,\mu=1）$$。

![3](/img/HANN/3.png)

从上图中，我们可以看出，使用了文档向量可以有效提高触发词抽取性能，表明文档级的信息有利于事件抽取。DEEB-RNN2与DEEB-RNN相比，改变了准确率和召回率的平衡。这主要是由以下理由造成的,一方面，与DEEB-RNN相比，DEEB-RNN2使用了gold句子级的注意力信号，表明对那些含有触发词的句子给予了更多关注。所以，双向RNN模型通过文档向量快速的过滤掉那些含事件但没有显式触发词的句子。这意味着DEEB-RNN2识别的事件都是含显式触发词的。所以，DEEB-RNN2提高了触发词抽取的准确率。另一方面，上述策略可能会减少那些没有在训练集中出现的事件触发词，因此，那些句子无法识别出触发词，导致召回率偏低。但是，DEEB-RNN3展现了事件抽取上的最佳性能，表明词级别和句级别的gold注意力信号均有利于事件触发词抽取。

目前，神经网络模型（bi-lstm,cnn）在短句上的编码能力提升的很明显，但对于一个较长的段落或文档，bi-lstm 和cnn都受限于句子的长度。因此，结合双向RNN模型的层次注意力机制对文档进行编码，可以很好捕获文档级别的信息，对下游任务的性能提升有很大的帮助。

>本文参考
>
>[HAN for Document Classification 阅读和实现](https://zhuanlan.zhihu.com/p/54165155)
>
>[Document Embedding Enhanced Event Detection with Hierarchical and
>Supervised Attention](http://www.aclweb.org/anthology/P18-2066)









