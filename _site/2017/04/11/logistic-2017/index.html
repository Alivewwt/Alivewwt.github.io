<!DOCTYPE html>
<html lang="zh-cn">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="keywords"  content="">
    
    <title>Logistic回归 - Wwt-blog</title>

    <!-- Favicon -->
    <link rel="shortcut icon" href="/img/favicon.ico">
    
    <!-- Canonical URL -->
    <link rel="canonical" href="http://wuwt.me//2017/04/11/logistic-2017/">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    <!-- font-awesome CSS -->
    <link rel="stylesheet" href="http://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" type="text/css">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/hux-blog.min.css">

    <!-- Highlight CSS -->
    <link rel="stylesheet" href="/css/googlecode.css">

    <!-- ga & ba script hoook -->
    <script></script>
</head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">

    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Wwt-blog</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">首页</a>
                    </li>
                    <li>
                        <a href="/tags">标签</a>
                    </li>
                    <li>
                        <a href="/about">关于</a>
                    </li>
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    var __HuxNav__ = {
        close: function(){
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        },
        open: function(){
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }

    // Bind Event
    $toggle.addEventListener('click', function(e){
        if ($navbar.className.indexOf('in') > 0) {
            __HuxNav__.close()
        }else{
            __HuxNav__.open()
        }
    })

    /**
     * Since Fastclick is used to delegate 'touchstart' globally
     * to hack 300ms delay in iOS by performing a fake 'click',
     * Using 'e.stopPropagation' to stop 'touchstart' event from 
     * $toggle/$collapse will break global delegation.
     * 
     * Instead, we use a 'e.target' filter to prevent handler
     * added to document close HuxNav.  
     *
     * Also, we use 'click' instead of 'touchstart' as compromise
     */
    document.addEventListener('click', function(e){
        if(e.target == $toggle) return;
        if(e.target.className == 'icon-bar') return;
        __HuxNav__.close();
    })
</script>


    <!-- Post Header -->
<style type="text/css">
    header.intro-header{
        position: relative;
        background-image: url('/img/logistic/bg.jpg')
    }
</style>
<header class="intro-header" >
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <div class="tags">
                        
                        <a class="tag" href="/tags/#机器学习" title="机器学习">机器学习</a>
                        
                        <a class="tag" href="/tags/#logistic回归" title="logistic回归">logistic回归</a>
                        
                    </div>
                    <h1>Logistic回归</h1>
                    
                    
                    <h2 class="subheading">回归算法</h2>
                    
                    <span class="meta">Posted by Wwt on April 11, 2017</span>
                </div>
            </div>
        </div>
    </div>
</header>

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">
            <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

				<p>​	逻辑回归模型其实仅在线性回归的基础上，套用了一个逻辑函数， 但也就由于这个逻辑函数，使得逻辑回归模型成为了机器学习领域一颗耀眼的明星。</p>

<h3 id="section">逻辑回归模型</h3>

<p>​	回归是一种极易理解的模型，假设现在有一些数据点，我们用一条直线对这些点进行拟合（该线称为最佳拟合直线），这个拟合过程就称作回归。利用Logistic回归进行分类的主要思想是：根据现有的数据对分类边界线建立回归公式，以此进行分类。这里的“回归”一词源于最佳拟合，表示找到最佳拟合参数集。训练分类器时的做法就是寻找最佳拟合参数，使用的是最优化算法。</p>

<p>​	举个例子，回归就相当于$y=f(x)$，表明自变量x与因变量y的关系。最常见问题如医生治病时的望、闻、问、切，之后判定病人是否生病了或者生了什么病，其中的望闻问切就是获取自变量x，即特征数据，判断是否生病就相当于获取因变量y，即预测分类。</p>

<p>​	最简单的回归是线性回归，在此借用吴恩达的讲义，线性回归假设特征和结果满足线性关系。其实线性关系的表达能力非常强大，每个特征对结果的影响强弱可以由前面的参数体现，而且每个特征变量可以首先映射到一个函数，然后再参与线性计算。这样就可以表达特征与结果之间的非线性关系。我们用X1,X2,···Xn去描述feature里面的分量，比如x1=是否吸烟，x2=作息规律，等等，我们可以做出一个估计函数：</p>

<script type="math/tex; mode=display">h(x)=h_θ(x)=θ_0+θ_1x_1+θ_2x_2</script>

<p>θ在这儿称为参数，在这的意思是调整feature中每个分量的影响力，就是到底是吸烟还是作息规律对患脑瘤的影响更大。</p>

<p>如下图1所示，X为数据点——肿瘤大小，Y为观测值——是否是恶性肿瘤。通过构建线性回归模型，如$h_Θ(x)$所示，构建模型后，既可以根据肿瘤大小，预测是否为恶性肿瘤$h_Θ(x)≥0.5$为恶性，$h_Θ(x)＜0.5$为良性。</p>

<p><img src="/img/logistic/1.jpg" alt="1" /></p>

<p>​															图-1</p>

<p>​	然而线性回归的健壮性很差，例如在图1.b的数据上建立回归，因最右边噪点的存在，使回归模型在训练集上表现都很差。这主要是由于线性回归在整个实数域内敏感度一致，而分类范围，需要在[0,1]。逻辑回归就是一种减小预测范围，将预测值限定在[0,1]间的一种回归模型，其回归方程与回归曲线如图2所示。逻辑曲线在z=0时，十分敏感，在$z＞＞0​$或$z«0​$处，都不敏感，将预测值限定为(0,1)。逻辑回归其实仅为在线性回归的基础上，套用了一个逻辑函数。我们想要的函数应该是，能接受所有的输入然后预测出类别。例如，在两个类的情况下，上述函数输出0或1.或许你之前接触过这种性质的函数，该函数称为<strong>海威赛德阶跃函数</strong>，或者直接称为单位阶跃函数。然而海威赛德阶跃函数的问题在于：该函数在跳跃点上从0瞬间跳跃到1，这个瞬间跳跃过程很难处理。幸好另一个函数也有类似的性质，且在数学上更易处理，这就是Sigmoid函数。Sigmoid函数具体的计算公式如下：</p>

<p>​						<script type="math/tex">σ(z)=\frac{1}{1+e^{-z}}</script></p>

<p><img src="/img/logistic/2.jpg" alt="2" /></p>

<p>​															图-2</p>

<p>​	图-2给出了Sigmoid函数在坐标系下的曲线图。当x为0时，Sigmoid函数为0.5.随着x的增大，对应的Sigmoid值将逼近与1；而随着x的减小，Sigmoid值将逼近于0.</p>

<p>​	因此，为了实现Logistic回归分类器，我们可以在每个特征上都乘以一个回归系数，然后把所有的结果值都相加，将这个总和代入Sigmoid函数中，进而得到一个在范围0~1之间的数值。任何大于0.5的数据被分入1类，小于0.5即被归入0类。所以Logistic回归也可以被看成一种概率估计。</p>

<p>​	确定了分类器的函数形式之后，现在的问题变成了：最佳回归系数是多少？如何确定他们的大小？这个问题将在下一节解答。</p>

<h3 id="section-1">基于最优化方法的最佳回归系数确定</h3>

<p>​	Sigmoid函数的输入记为z，由下面公式得出：</p>

<p>​			<script type="math/tex">z=w_0x_0+w_1x_1+···+w_nx_n</script></p>

<p>​	如果采用向量的写法，上述公式可以写成$z=w^Tx$,它表示将这两个数值向量对应元素相乘然后全部加起来即得到z值。其中的向量x是分类器的输入数据，向量w也就是我们要找到的最佳参数(系数),从而使得分类器尽可能地精确。为了寻找该最佳参数，需要用到最优化理论的一些知识。下面首先介绍梯度上升这一最优化方法，我们将学习到如何使用该方法求得数据集的最佳参数。最后我们将学习随机梯度上升算法，以及如何对其修改以获得更好的结果。</p>

<h4 id="section-2">梯度上升法</h4>

<p>​	这里介绍的第一个最优化算法叫做梯度上升法。梯度上升法基于的思想是：要找到某函数的最大值，最好的方法是沿着该函数的梯度方向探寻。如果梯度记为▽，则函数f(x,y)的梯度由下式表示：</p>

<p>​	<script type="math/tex">▽f(x,y)=〔\frac{δf(x,y)}{δx}    \frac{δf(x,y)}{δy}   〕</script></p>

<p>​	这是机器学习中最易造成混淆的一个地方，但在数学上并不难，需要做的只是牢记这些符号的意义。这个梯度意味着要沿x的方向移动$\frac{δf(x,y)}{δx} $,沿y的方向移动$\frac{δf(x,y)}{δy}$。其中，函数f(x,y)必须要在待计算的点上有定义并且可微。一个具体的函数例子如图-3所示。</p>

<p><img src="/img/logistic/3.jpg" alt="3" /></p>

 														图-3

<p>​	梯度上升算法到达每个点后都会重新估计移动方向。从$x_0$开始，计算完该点的梯度，函数就根据梯度移动到下一点$x_1$。在$x_1$点，梯度再次被重新计算，并沿着新的梯度方向移动到$x_2$。如此循环迭代，直到满足停止条件。迭代的过程中，梯度算子总是保证我们能选取到最佳的移动方向。</p>

<p>​	图-3中的梯度上升算法沿梯度方向移动了一步。可以看到梯度算子总是指向函数值增长最快的方向。这里说的是移动方向，而未提到移动量的大小。该量值称为步长，记做α。用向量来表示的话，梯度上升算法的迭代公式如下：</p>

<p>​	<script type="math/tex">w:=w+α▽_wf(w)</script></p>

<p>​	该公式将一直被迭代执行，直至达到某个停止条件为止，比如迭代次数达到某个指定值或者算法达到某个可以允许的误差范围。</p>

<p><strong>梯度下降算法</strong></p>

<p>我们经常听到的应该是梯度下降算法，它与这里的梯度上升算法一样，只是公式中加法需要变成减法。因此，对应的公式可以写成</p>

<p>​	<script type="math/tex">w:=w0-α▽_wf(w)</script></p>

<p>梯度上升算法用来求函数的最大值，而梯度下降算法用来求函数的最小值。</p>

<h4 id="section-3">训练算法：使用梯度上升找到最佳参数</h4>

<p>​	梯度上升法的伪代码如下：</p>

<p>​	<strong>每个回归系数初始化为1</strong></p>

<p>​	<strong>重复R次:</strong></p>

<p>​	<strong>计算整个数据集的梯度</strong></p>

<p>​	<strong>使用alpha×gradient更新回归系数的向量</strong></p>

<p>​	<strong>返回回归系数</strong></p>

<p>下面的代码是梯度上升算法的具体实现。</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="o">*</span>
<span class="s">'''
主要功能打开文件test.txt并逐行读取。
每行前两个值分别是X1和X2，第三个值是数据对应的类别标签
'''</span>
<span class="k">def</span> <span class="nf">loadDataSet</span><span class="p">():</span>
	<span class="n">dataMat</span><span class="o">=</span><span class="p">[];</span>
	<span class="n">labelMat</span><span class="o">=</span><span class="p">[]</span>
	<span class="n">fr</span><span class="o">=</span><span class="nb">open</span><span class="p">(</span><span class="s">'testSet.txt'</span><span class="p">)</span>
	<span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">fr</span><span class="o">.</span><span class="n">readlines</span><span class="p">():</span>
		<span class="n">lineArr</span><span class="o">=</span><span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
		<span class="n">dataMat</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span><span class="nb">float</span><span class="p">(</span><span class="n">lineArr</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span><span class="nb">float</span><span class="p">(</span><span class="n">lineArr</span><span class="p">[</span><span class="mi">1</span><span class="p">])])</span>
		<span class="n">labelMat</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">lineArr</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
	<span class="k">return</span> <span class="n">dataMat</span><span class="p">,</span><span class="n">labelMat</span>

<span class="k">def</span> <span class="nf">sigmod</span><span class="p">(</span><span class="n">inX</span><span class="p">):</span>
	<span class="k">return</span> <span class="mf">1.0</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">inX</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">gradAscent</span><span class="p">(</span><span class="n">dataMatIn</span><span class="p">,</span><span class="n">classLabels</span><span class="p">):</span>
	<span class="c">#转换为Numpy矩阵数据类型</span>
	<span class="n">dataMatrix</span><span class="o">=</span><span class="n">mat</span><span class="p">(</span><span class="n">dataMatIn</span><span class="p">)</span>
	<span class="n">labelMat</span><span class="o">=</span><span class="n">mat</span><span class="p">(</span><span class="n">classLabels</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>
	<span class="n">m</span><span class="p">,</span><span class="n">n</span><span class="o">=</span><span class="n">shape</span><span class="p">(</span><span class="n">dataMatrix</span><span class="p">)</span>
	<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>
	<span class="n">alpha</span><span class="o">=</span><span class="mf">0.001</span> <span class="c">#alpha是向目标移动的步长</span>
	<span class="n">maxCycles</span><span class="o">=</span><span class="mi">500</span> <span class="c">#迭代次数</span>
	<span class="c">#生成n*1的1矩阵</span>
	<span class="n">weights</span><span class="o">=</span><span class="n">ones</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
	<span class="c">#矩阵相乘</span>
	<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">maxCycles</span><span class="p">):</span>
		<span class="n">h</span><span class="o">=</span><span class="n">sigmod</span><span class="p">(</span><span class="n">dataMatrix</span><span class="o">*</span><span class="n">weights</span><span class="p">)</span>
		<span class="n">error</span><span class="o">=</span><span class="p">(</span><span class="n">labelMat</span><span class="o">-</span><span class="n">h</span><span class="p">)</span>
		<span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="o">+</span><span class="n">alpha</span><span class="o">*</span><span class="n">dataMatrix</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span><span class="o">*</span><span class="n">error</span>
	<span class="k">return</span> <span class="n">weights</span>
</code></pre>
</div>

<h4 id="section-4">随机梯度上升</h4>

<p>​	梯度上升算法在每次更新回归系数时都要遍历整个数据集，该方法在处理100个左右的数据集尚可，但如果有数十亿样本和成千上万的特征，那么该方法的计算复杂度就太高了。一种改进方法是一次仅用一个样本点来更新回归系数，该方法称为“随机梯度上升算法”。由于可以在新样本到来时对分类器进行增量式更新，因而随机梯度上升算法是一个在线学习算法。与“在线学习”相对应，一次处理所有数据被称作为“批处理”。</p>

<p>​	随机梯度上升算法可以写成如下的伪代码：</p>

<p>​	所有回归系数初始化为1</p>

<p>​	<strong>对数据集中每个样本</strong></p>

<p>​			<strong>计算该样本的梯度</strong></p>

<p>​			<strong>使用alpha×gradient更新回归系数值</strong></p>

<p>​	<strong>返回回归系数值</strong></p>

<p>​	以下是随机梯度上升算法的实现代码。</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="err">随机梯度上升</span>
<span class="k">def</span> <span class="nf">stocGradAscent0</span><span class="p">(</span><span class="n">dataMatrix</span><span class="p">,</span><span class="n">classLabels</span><span class="p">):</span>
	<span class="n">m</span><span class="p">,</span><span class="n">n</span><span class="o">=</span><span class="n">shape</span><span class="p">(</span><span class="n">dataMatrix</span><span class="p">)</span>
	<span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span>
	<span class="n">weights</span><span class="o">=</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
	<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
		<span class="n">h</span><span class="o">=</span><span class="n">sigmod</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">dataMatrix</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">weights</span><span class="p">))</span>
		<span class="n">error</span><span class="o">=</span><span class="n">classLabels</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="n">h</span>
		<span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="o">+</span><span class="n">alpha</span><span class="o">*</span><span class="n">error</span><span class="o">*</span><span class="n">dataMatrix</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
	<span class="k">return</span> <span class="n">weights</span>
</code></pre>
</div>

<h3 id="section-5">小结</h3>

<p>​	Logistic回归的目的是寻找一个非线性函数Sigmoid的最佳拟合参数，求解过程可以由最优化算法来完成。在最优化算法中，最常用的就是梯度上升算法，而梯度上升算法又可以简化为随机梯度上升算法。</p>

<p>​	随机梯度上升算法与梯度上升算法的效果相当，但占用更少的计算资源。此外，随机梯度上升是一个在线算法，它可以在新数据到来时完成参数更新，而不需要重新读取整个数据集来进行批处理运算。</p>

<h3 id="section-6">参考</h3>

<p><a href="http://www.cnblogs.com/jerrylead/archive/2011/03/05/1971867.html">对线性回归，logistic回归和一般回归的认识</a></p>

<p><a href="http://www.tuicool.com/articles/auQFju">逻辑回归模型(Logistic Regression, LR)基础</a></p>

<p><a href="">机器学习实战</a></p>


                <hr style="visibility: hidden;">

                <!-- Baidu Share -->
                
                <div class="bdsharebuttonbox"><a href="#" class="bds_more" data-cmd="more">分享到：</a><a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信">微信</a><a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博">新浪微博</a><a href="#" class="bds_douban" data-cmd="douban" title="分享到豆瓣网">豆瓣网</a></div>
                

                <hr>
                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/2017/03/22/HMM-2017/" data-toggle="tooltip" data-placement="top" title="隐马尔科夫模型">前一篇<br>
                        <span>隐马尔科夫模型</span>
                        </a>
                    </li>
                    
                    
                </ul>

                <hr>

                <!-- Netease Gentie -->
                
            </div>  

    <!-- Side Catalog Container -->
        
            <div class="
                col-lg-2 col-lg-offset-0
                visible-lg-block
                sidebar-container
                catalog-container">
                <div class="side-catalog">
                    <hr class="hidden-sm hidden-xs">
                    <h5>
                        <a class="catalog-toggle" href="#">目录</a>
                    </h5>
                    <ul class="catalog-body"></ul>
                </div>
            </div>
        
            </div>
        </div>
    </div>
</article>


<!-- Baidu Share -->
<script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"1","bdSize":"16"},"share":{"bdSize":16}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
</script>



<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("//cdnjs.cloudflare.com/ajax/libs/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'always',
          placement: 'right',
          icon: '#'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>
<style>
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>


    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                    <!-- add Weibo, Zhihu by Hux, add target = "_blank" to <a> by Hux -->
                    
                    <li>
                        <a target="_blank" href="https://www.zhihu.com/people/ling-du-qing-xu">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa  fa-stack-1x fa-inverse">知</i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li>
                        <a target="_blank" href="http://weibo.com/3537251257">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-weibo fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li>
                        <a target="_blank" href="https://github.com/Alivewwt">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    
                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; Wwt-blog 2017
                </p>
            </div>
        </div>
    </div>
</footer>

<!-- MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" async
  src="http://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML">
</script>

<!-- jQuery -->
<script src="/js/jquery.min.js "></script>

<!-- Bootstrap Core JavaScript -->
<script src="/js/bootstrap.min.js "></script>

<!-- Custom Theme JavaScript -->
<script src="/js/hux-blog.min.js "></script>

<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>

<!-- Highlight.js -->
<script>
    async("http://cdn.bootcss.com/highlight.js/9.9.0/highlight.min.js", function(){
        hljs.initHighlightingOnLoad();
    })
</script>

<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async('/js/jquery.tagcloud.js',function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("//cdnjs.cloudflare.com/ajax/libs/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>

<!-- Baidu Push-->
<script>
    // do not push in tag.html
    if($('#tag_cloud').length == 0){
        (function(){
            var bp = document.createElement('script');
            var curProtocol = window.location.protocol.split(':')[0];
            if (curProtocol === 'https') {
                bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
            }
            else {
                bp.src = 'http://push.zhanzhang.baidu.com/push.js';
            }
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(bp, s);
        })();
    }
</script>

<!-- Baidu Tongji -->

<script>
    // dynamic User by Hux
    var _baId = '07529a93af5fa9f808261a1318c3aba4';

    // Originial
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?" + _baId;
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
</script>


<!-- Side Catalog -->

<script type="text/javascript">
    function generateCatalog (selector) {
        var P = $('div.post-container'),a,n,t,l,i,c;
        a = P.find('h1,h2,h3,h4,h5,h6');
        a.each(function () {
            n = $(this).prop('tagName').toLowerCase();
            i = "#"+$(this).prop('id');
            t = $(this).text();
            c = $('<a href="'+i+'" rel="nofollow">'+t+'</a>');
            l = $('<li class="'+n+'_nav"></li>').append(c);
            $(selector).append(l);
        });
        return true;    
    }

    generateCatalog(".catalog-body");

    // toggle side catalog
    $(".catalog-toggle").click((function(e){
        e.preventDefault();
        $('.side-catalog').toggleClass("fold")
    }))

    /*
     * Doc: https://github.com/davist11/jQuery-One-Page-Nav
     * Fork by Hux to support padding
     */
    async("/js/jquery.nav.js", function () {
        $('.catalog-body').onePageNav({
            currentClass: "active",
            changeHash: !1,
            easing: "swing",
            filter: "",
            scrollSpeed: 700,
            scrollOffset: 0,
            scrollThreshold: .2,
            begin: null,
            end: null,
            scrollChange: null,
            padding: 80
        });
    });
</script>



<!-- Image to hack wechat -->
<img src="/img/icon_wechat.jpg" width="0" height="0" />
<!-- Migrate from head to bottom, no longer block render and still work -->

</body>

</html>
