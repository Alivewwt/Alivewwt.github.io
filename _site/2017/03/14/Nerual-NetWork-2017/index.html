<!DOCTYPE html>
<html lang="zh-cn">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="keywords"  content="">
    
    <title>神经网络 - Wwt-blog</title>

    <!-- Favicon -->
    <link rel="shortcut icon" href="/img/favicon.ico">
    
    <!-- Canonical URL -->
    <link rel="canonical" href="http://wuwt.me//2017/03/14/Nerual-NetWork-2017/">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    <!-- font-awesome CSS -->
    <link rel="stylesheet" href="http://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" type="text/css">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/hux-blog.min.css">

    <!-- Highlight CSS -->
    <link rel="stylesheet" href="/css/googlecode.css">

    <!-- ga & ba script hoook -->
    <script></script>
</head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">

    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Wwt-blog</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">首页</a>
                    </li>
                    <li>
                        <a href="/tags">标签</a>
                    </li>
                    <li>
                        <a href="/about">关于</a>
                    </li>
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    var __HuxNav__ = {
        close: function(){
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        },
        open: function(){
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }

    // Bind Event
    $toggle.addEventListener('click', function(e){
        if ($navbar.className.indexOf('in') > 0) {
            __HuxNav__.close()
        }else{
            __HuxNav__.open()
        }
    })

    /**
     * Since Fastclick is used to delegate 'touchstart' globally
     * to hack 300ms delay in iOS by performing a fake 'click',
     * Using 'e.stopPropagation' to stop 'touchstart' event from 
     * $toggle/$collapse will break global delegation.
     * 
     * Instead, we use a 'e.target' filter to prevent handler
     * added to document close HuxNav.  
     *
     * Also, we use 'click' instead of 'touchstart' as compromise
     */
    document.addEventListener('click', function(e){
        if(e.target == $toggle) return;
        if(e.target.className == 'icon-bar') return;
        __HuxNav__.close();
    })
</script>


    <!-- Post Header -->
<style type="text/css">
    header.intro-header{
        position: relative;
        background-image: url('/img/Nerual/nerual.jpg')
    }
</style>
<header class="intro-header" >
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <div class="tags">
                        
                        <a class="tag" href="/tags/#神经网络" title="神经网络">神经网络</a>
                        
                        <a class="tag" href="/tags/#深度学习" title="深度学习">深度学习</a>
                        
                    </div>
                    <h1>神经网络</h1>
                    
                    
                    <h2 class="subheading">后向传播算法</h2>
                    
                    <span class="meta">Posted by Wwt on March 14, 2017</span>
                </div>
            </div>
        </div>
    </div>
</header>

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">
            <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

				<p>​	最近在看张重生著的《深度学习》这本书，看到神经网络这一章，顿时恍然大悟，发现自己对神经网络这一块有了新的理解和感受。于是写下这篇笔记，加深自己的体会。</p>

<p>​	神经网络的神经网络是一种经典的机器学习算法，随着对神经网络研究的不断深入，目前，它在模式识别、物体检测、视频分析和图像识别领域等领域发挥着越来越重要的作用。</p>

<h2 id="section">1.神经元的概念</h2>

<p>​	神经元是构成神经网络的额基本单元，其基本模型如图(1)所示。</p>

<p><img src="/img/Nerual/timg.jpg" alt="timg" /></p>

<p>​								图(1)</p>

<p>可以表示为<script type="math/tex">y_{k}=f(\sum_{i=1}^n w_{ik}×x_{i}+b_{k})</script></p>

<p>其中$f(·)$可以是sigmoid函数（$f(x)=\frac{1}{1+e^{-x}}$,域值范围为[0,1]),也可以双曲线正切</p>

<p>($f(x)=\frac{e^{x}-e^{x}}{e^{x}+e^{-x}}$)，域值范围为[-1,1]）等非线性函数。sigmoid函数被看成一个挤压函数，它可以将一个较大输入范围挤压到较小的0~1的区域。sigmoid函数和双曲线正切函数的函数图像见图(2)。$y_{k}$的值是由输入和对应的权值进行线性求和，然后再加上偏置值$b_{k}$，在使用激活函数$f(·)$获得的。</p>

<p><img src="/img/Nerual/sigmoid.jpg" alt="sigmoid" /></p>

<p><img src="/img/Nerual/tan.jpg" alt="tan" /></p>

<p>​										图(2)</p>

<h2 id="section-1">2.神经网络</h2>

<p>​	人工神经网络（Artificial Nerual Network,ANN）是多层感知器。神经网络包括输入层、一层或若干隐藏层以及输出层，如下图所示。该多层网络结构包括1个输入层、2个隐藏层和1个输出层，其基本组成单元为神经单元，如$x_1$等。</p>

<p><img src="/img/Nerual/structure.jpg" alt="structure" /></p>

<p>​							图(3)</p>

<p>​	如果神经网络中的隐藏层足够多，该神经网络可以逼近任何函数。但由于网络结构比较负载，会造成过拟合现象，或在误差后向传播过程中产生梯度弥散现象。</p>

<h3 id="section-2">2.1后向传播算法</h3>

<p>​	神经网络的训练包括正向传播和反向传播两个过程。正向传播是输入信号从输入层，经过若干隐藏层，从输出层输出结果（预测）的过程；误差后向传播算法是将误差信号从输出层反向传播至输入层的过程。反向传播主要使用误差后向传播（EBP）算法和梯度下降对网络各层调整权重，通过比较输出信号和期望信号得到误差信号，利用链式求导将误差信号逐层向前传播得到各层误差信号，根据各层误差信号调整各层权重和相关参数。而不断调整权重和相关参数的过程就是人工神经网络训练学习的过程。</p>

<p>​	我们希望网络的输出尽可能的接近真正想要的预测值。那么就可以通过比较当前网络的预测值和我们想要的目标值，再根据两者的差异情况来更新每一层的权重矩阵（比如，如果网络的预测值高了，就调整权重让它预测低一些，不断调整，指导能够预测出目标值）。因此就需要先定义“如何比较预测值和目标值的差异”，这便是<strong>损失函数</strong>或<strong>目标函数</strong>，用于衡量预测值和目标值的差异方程。损失函数的输出值越高表示差异性越大。那神经网络的训练就变成了尽可能的缩小损失的过程。所用的方法是<strong>梯度下降</strong>：通过使损失值向当前点对于梯度的反方向不断移动，来降低损失。一次移动多少是由<strong>学习速率</strong>来控制的。</p>

<p>​	假设第i个单元与第j个单元有链接，则相关符号说明如下：</p>

<p>​	$b_j$ 表示与第j个单元相关的偏置值；</p>

<p>​	$I_j=（\sum_{i}w_{ij}O_j+b_j）$表示第j个单元的输入值；</p>

<p>​	$O_j$表示第j个单元的输出值；</p>

<p>​	$f(·)$表示神经元激活函数；</p>

<p>​	$w_{ij}$表示第i个单元与第j个单元之间的权值；</p>

<p>​	$b_i$表示与第i个神经单元相关的偏置值；</p>

<p>​	$λ$表示学习速率；</p>

<p>​	$δ_{i}$表示与第i个神经元相关的误差项；</p>

<p>​	$nextlayer(j)$表示下一层中与第j个神经单元相连的神经单元集合。</p>

<p>​	使用sigmoid激活函数的神经网络的后向传播算法整体思想如下。</p>

<p>​	while不满足终止条件{</p>

<p>​	遍历训练集中的每一个样例：</p>

<p>​	<strong>#1.将每个训练样例输入沿着网络向前传播计算：</strong></p>

<p>​	输入层没有计算操作，所以输入层的输出等于输入：</p>

<p>​							$O_j=I_j$			                                                                          (3.2)</p>

<p>​	隐藏层或输出层输出：</p>

<p>​	$I_j=（\sum_{i}w_{ij}O_j+b_j）$ //根据与单元j相连的单元，计算单元j的输入</p>

<p>​		$O_j=f(I_j)$//使用激活函数(sigmoid函数)计算单元j的输出			                            (3.3)</p>

<p>​	<strong>#2.使用误差项后向传播</strong></p>

<p>​	输出单元j误差项的计算：</p>

<p>​				<script type="math/tex">δ_j=O_j×(1-O_j)×(T_j-O_j)</script>		                                                               (3.4)</p>

<p>​	隐藏单元j误差项的计算：</p>

<p>​			<script type="math/tex">δ_j=O_j×(1-O_j)×\sum_{k∈nextplayers}×δ_k×w_jk</script>//k为下一层与隐藏单元j相连的单元 (3.5)</p>

<p>​	<strong>权重更新</strong></p>

<p>​			<script type="math/tex">△w_{ij}=λ×δ_{j}×O_i​</script></p>

<p>​			<script type="math/tex">w_{ij}=w_{ij}+△w_{ij}</script>														(3.6)</p>

<p>​	<strong>偏置更新</strong></p>

<p>​			<script type="math/tex">△b_j=λ×δ_j</script></p>

<p>​			b_j=b_j+△b_j																(3.7)</p>

<p>}</p>

<p>​	上面算法中的终止条件为认为设定条件，可以是达到指定的迭代次数，也可以是网络收敛到一定程度等。终止条件的选择至关重要，例如终止条件为迭代次数时，迭代次数过少，网络误差降低幅度很小；迭代次数过多，有可能会导致网络出现过拟合现象。</p>

<h3 id="section-3">2.2神经网络算法示例</h3>

<p>​	此处是要介绍后向传播算法的推导过程，让我们能更清晰地了解权值和偏置更新。但由于推导过程是纯数学推导过程，这里就不一一推导了，有兴趣的自己可以去网上搜搜看。我在这里主要讲述神经网络算法的详细计算过程。</p>

<p>​	这里我们讲解单样本输入时的计算过程。设输入样例为${x_1.x_2,x_3,x_4}={1,0,1,1}$,学习速率$l=0.8$,真实标签为1，神经网络结构示例如上图(3)所示，网络中相关权重和偏置信息如表2-1所示。</p>

<p>​								<strong>表2-1网络中相关权重和偏置信息</strong></p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">$w_{15}$</th>
      <th style="text-align: left">0.5</th>
      <th style="text-align: left">$w_{46}$</th>
      <th style="text-align: left">0.5</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">$w_{16}$</td>
      <td style="text-align: left">0.2</td>
      <td style="text-align: left">$w_{47}$</td>
      <td style="text-align: left">-0.1</td>
    </tr>
    <tr>
      <td style="text-align: left">$w_{17}$</td>
      <td style="text-align: left">-0.5</td>
      <td style="text-align: left">$w_{58}$</td>
      <td style="text-align: left">0.6</td>
    </tr>
    <tr>
      <td style="text-align: left">$w_{25}$</td>
      <td style="text-align: left">-0.2</td>
      <td style="text-align: left">$w_{68}$</td>
      <td style="text-align: left">-0.2</td>
    </tr>
    <tr>
      <td style="text-align: left">$w_{26}$</td>
      <td style="text-align: left">0.3</td>
      <td style="text-align: left">$w_{78}$</td>
      <td style="text-align: left">0.1</td>
    </tr>
    <tr>
      <td style="text-align: left">$w_{27}$</td>
      <td style="text-align: left">0.4</td>
      <td style="text-align: left">$b_{5}$</td>
      <td style="text-align: left">0.2</td>
    </tr>
    <tr>
      <td style="text-align: left">$w_{35}$</td>
      <td style="text-align: left">0.8</td>
      <td style="text-align: left">$b_{6}$</td>
      <td style="text-align: left">-0.5</td>
    </tr>
    <tr>
      <td style="text-align: left">$w_{36}$</td>
      <td style="text-align: left">-0.6</td>
      <td style="text-align: left">$b_7$</td>
      <td style="text-align: left">0.4</td>
    </tr>
    <tr>
      <td style="text-align: left">$w_{37}$</td>
      <td style="text-align: left">0.4</td>
      <td style="text-align: left">$b_8$</td>
      <td style="text-align: left">0.6</td>
    </tr>
    <tr>
      <td style="text-align: left">$w_{45}$</td>
      <td style="text-align: left">0.6</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
  </tbody>
</table>

<p>​	根据式(3.2)和式(3.3)可以计算网络中每个单元的输入和输出值，如表2-2所示。</p>

<p>​								<strong>表2-2网络每个单元的输入和输出</strong></p>

<table>
  <thead>
    <tr>
      <th>神经单元</th>
      <th style="text-align: left">输入I</th>
      <th>输出O</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$x_1$</td>
      <td style="text-align: left">1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>$x_2$</td>
      <td style="text-align: left">0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>$x_3$</td>
      <td style="text-align: left">1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>$x_4$</td>
      <td style="text-align: left">1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>$x_5$</td>
      <td style="text-align: left">1×0.5+0×(-0.2)+1×0.8+1×0.6+0.2=2.1</td>
      <td>$\frac{1}{1+e^{-2.1}}=0.8909$</td>
    </tr>
    <tr>
      <td>$x_6$</td>
      <td style="text-align: left">1×0.2+0×0.3+1×(-0.6)+1×0.5-0.5=0.4</td>
      <td>$\frac{1}{1+e^{0.4}}=0.40131$</td>
    </tr>
    <tr>
      <td>$x_7$</td>
      <td style="text-align: left">1×(-0.5)+0×0.4+1×0.4+1×(-0.1)+0.4=0.2</td>
      <td>$\frac{1}{1+e^{-0.2}}=0.54983$</td>
    </tr>
    <tr>
      <td>$x_8$</td>
      <td style="text-align: left">0.8909×0.6+0.40131×(-0.2)+0.54983×0.1+0.6=1.1093</td>
      <td>$\frac{1}{1+e^{-1.1093}}=0.752$</td>
    </tr>
  </tbody>
</table>

<p>​	根据式(3.4)和式(3.5)可以计算网络中每个单元的误差项的值，如表2-3所示。</p>

<p>​							<strong>表2-3  网络中每个单元的误差项的值</strong></p>

<table>
  <thead>
    <tr>
      <th>神经单元</th>
      <th>误差项$δ$</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$δ_{x8}$</td>
      <td>0.752×(1-0.752)×(1-0.752)=0.04625</td>
    </tr>
    <tr>
      <td>$δ_{x7}$</td>
      <td>0.54982×(1-0.54983)×(0.1×0.04625)=0.0011448</td>
    </tr>
    <tr>
      <td>$δ_{x6}$</td>
      <td>0.40131×(1-0.40131)×(-0.2×0.04625)=-0.0022224</td>
    </tr>
    <tr>
      <td>$δ_{x5}$</td>
      <td>0.8909×(1-0.8909)×(0.6×0.04625)=0.0026972</td>
    </tr>
  </tbody>
</table>

<p>​	根据式(3.6)和式(3.7)可以计算网络中更新后的权重值和偏置值，如表2-4</p>

<p>​							<strong>表2-4  网络中更新后的权重值和偏置值</strong></p>

<table>
  <thead>
    <tr>
      <th>权重和偏置</th>
      <th>更新后的值</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$w_{15}$</td>
      <td>0.5+0.8×0.0026972×1=0.50216</td>
    </tr>
    <tr>
      <td>$w_{16}$</td>
      <td>0.2+0.8×(-0.0022224)×1=0.19822</td>
    </tr>
    <tr>
      <td>$w_{17}$</td>
      <td>(-0.5)+0.8×0.0011448×1=0.49908</td>
    </tr>
    <tr>
      <td>$w_{25}$</td>
      <td>(-0.2)+0.8×0.0026972×0=-0.2</td>
    </tr>
    <tr>
      <td>$w_{26}$</td>
      <td>0.3+0.8×(-0.002224)×0=0.3</td>
    </tr>
    <tr>
      <td>$w_{27}$</td>
      <td>0.4+0.8×0.0011448×0=0.4</td>
    </tr>
    <tr>
      <td>$w_{35}$</td>
      <td>0.8+0.8×0.0026972×1=0.80216</td>
    </tr>
    <tr>
      <td>$w_{36}$</td>
      <td>(-0.6)+0.8×(-0.0022224)×1=-0.60178</td>
    </tr>
    <tr>
      <td>$w_{37}$</td>
      <td>0.4+0.8×0.0011448×1=0.40092</td>
    </tr>
    <tr>
      <td>$w_{45}$</td>
      <td>0.6+0.8×0.0026972×1=0.60216</td>
    </tr>
    <tr>
      <td>$w_{46}$</td>
      <td>0.5+0.8×(-0.002224)×1=0.49822</td>
    </tr>
    <tr>
      <td>$w_{47}$</td>
      <td>(-0.1)+0.8×0.0011448×1=-0.099084</td>
    </tr>
    <tr>
      <td>$w_{58}$</td>
      <td>0.6+0.8×0.04625×0.8909=0.63296</td>
    </tr>
    <tr>
      <td>$w_{68}$</td>
      <td>(-0.2)+0.8×0.04625×0.40131=-0.18515</td>
    </tr>
    <tr>
      <td>$w_{78}$</td>
      <td>0.1+0.8×0.04625×0.54983=0.12034</td>
    </tr>
    <tr>
      <td>$b_{5}$</td>
      <td>0.2+0.8×0.0026972=0.20216</td>
    </tr>
    <tr>
      <td>$b_{6}$</td>
      <td>(-0.5)+0.8×(-0.0022224)=-0.50178</td>
    </tr>
    <tr>
      <td>$b_{7}$</td>
      <td>0.4+0.8×0.0011448=0.40092</td>
    </tr>
    <tr>
      <td>$b_{8}$</td>
      <td>0.6+0.8×0.04625=0.637</td>
    </tr>
  </tbody>
</table>

<h2 id="section-4">3.参考内容</h2>

<p>​	<a href="">《深度学习-原理与应用理论》</a></p>


                <hr style="visibility: hidden;">

                <!-- Baidu Share -->
                
                <div class="bdsharebuttonbox"><a href="#" class="bds_more" data-cmd="more">分享到：</a><a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信">微信</a><a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博">新浪微博</a><a href="#" class="bds_douban" data-cmd="douban" title="分享到豆瓣网">豆瓣网</a></div>
                

                <hr>
                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/2017/03/12/SSM-2017/" data-toggle="tooltip" data-placement="top" title="Spring-SpringMVC-MyBatis">前一篇<br>
                        <span>Spring-SpringMVC-MyBatis</span>
                        </a>
                    </li>
                    
                    
                    <li class="next">
                        <a href="/2017/03/22/HMM-2017/" data-toggle="tooltip" data-placement="top" title="隐马尔科夫模型">后一篇<br>
                        <span>隐马尔科夫模型</span>
                        </a>
                    </li>
                    
                </ul>

                <hr>

                <!-- Netease Gentie -->
                
            </div>  

    <!-- Side Catalog Container -->
        
            </div>
        </div>
    </div>
</article>


<!-- Baidu Share -->
<script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"1","bdSize":"16"},"share":{"bdSize":16}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
</script>



<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("//cdnjs.cloudflare.com/ajax/libs/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'always',
          placement: 'right',
          icon: '#'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>
<style>
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>


    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                    <!-- add Weibo, Zhihu by Hux, add target = "_blank" to <a> by Hux -->
                    
                    <li>
                        <a target="_blank" href="https://www.zhihu.com/people/ling-du-qing-xu">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa  fa-stack-1x fa-inverse">知</i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li>
                        <a target="_blank" href="http://weibo.com/3537251257">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-weibo fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li>
                        <a target="_blank" href="https://github.com/Alivewwt">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    
                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; Wwt-blog 2017
                </p>
            </div>
        </div>
    </div>
</footer>

<!-- MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" async
  src="http://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML">
</script>

<!-- jQuery -->
<script src="/js/jquery.min.js "></script>

<!-- Bootstrap Core JavaScript -->
<script src="/js/bootstrap.min.js "></script>

<!-- Custom Theme JavaScript -->
<script src="/js/hux-blog.min.js "></script>

<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>

<!-- Highlight.js -->
<script>
    async("http://cdn.bootcss.com/highlight.js/9.9.0/highlight.min.js", function(){
        hljs.initHighlightingOnLoad();
    })
</script>

<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async('/js/jquery.tagcloud.js',function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("//cdnjs.cloudflare.com/ajax/libs/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>

<!-- Baidu Push-->
<script>
    // do not push in tag.html
    if($('#tag_cloud').length == 0){
        (function(){
            var bp = document.createElement('script');
            var curProtocol = window.location.protocol.split(':')[0];
            if (curProtocol === 'https') {
                bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
            }
            else {
                bp.src = 'http://push.zhanzhang.baidu.com/push.js';
            }
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(bp, s);
        })();
    }
</script>

<!-- Baidu Tongji -->

<script>
    // dynamic User by Hux
    var _baId = '07529a93af5fa9f808261a1318c3aba4';

    // Originial
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?" + _baId;
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
</script>


<!-- Side Catalog -->



<!-- Image to hack wechat -->
<img src="/img/icon_wechat.jpg" width="0" height="0" />
<!-- Migrate from head to bottom, no longer block render and still work -->

</body>

</html>
